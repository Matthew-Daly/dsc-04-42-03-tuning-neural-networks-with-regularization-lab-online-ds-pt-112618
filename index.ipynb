{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to usex\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 2 columns):\n",
      "Product                         60000 non-null object\n",
      "Consumer complaint narrative    60000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 937.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df['Product']\n",
    "complaints = df['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences type <class 'list'>\n",
      "one_hot_results type: <class 'numpy.ndarray'>\n",
      "Found 21363 unique tokens.\n",
      "Dimensions of our coded results: (10000, 2000)\n"
     ]
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "print('sequences type', type(sequences))\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "print('one_hot_results type:', type(one_hot_results))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "print('Dimensions of our coded results:', np.shape(one_hot_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_categories = le.transform(product)\n",
    "\n",
    "product_one_hot = to_categorical(product_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test =  train_test_split(one_hot_results, product_one_hot, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0630 01:01:11.334589 140611292423936 deprecation_wrapper.py:119] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0630 01:01:11.452190 140611292423936 deprecation_wrapper.py:119] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0630 01:03:18.705300 140611292423936 deprecation_wrapper.py:119] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0630 01:03:18.779345 140611292423936 deprecation_wrapper.py:119] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0630 01:04:06.140840 140611292423936 deprecation.py:323] From /home/matthew/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0630 01:04:06.196490 140611292423936 deprecation_wrapper.py:119] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 2s 236us/step - loss: 1.9427 - acc: 0.1931 - val_loss: 1.9352 - val_acc: 0.2140\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9231 - acc: 0.2169 - val_loss: 1.9180 - val_acc: 0.2300\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9046 - acc: 0.2411 - val_loss: 1.9007 - val_acc: 0.2450\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8853 - acc: 0.2563 - val_loss: 1.8812 - val_acc: 0.2620\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8633 - acc: 0.2779 - val_loss: 1.8588 - val_acc: 0.2770\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8373 - acc: 0.2963 - val_loss: 1.8327 - val_acc: 0.2970\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8076 - acc: 0.3165 - val_loss: 1.8030 - val_acc: 0.3180\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7744 - acc: 0.3364 - val_loss: 1.7704 - val_acc: 0.3390\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7381 - acc: 0.3568 - val_loss: 1.7350 - val_acc: 0.3530\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6982 - acc: 0.3797 - val_loss: 1.6963 - val_acc: 0.3740\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6555 - acc: 0.3985 - val_loss: 1.6549 - val_acc: 0.4010\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6108 - acc: 0.4269 - val_loss: 1.6118 - val_acc: 0.4280\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5646 - acc: 0.4528 - val_loss: 1.5679 - val_acc: 0.4520\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5172 - acc: 0.4768 - val_loss: 1.5225 - val_acc: 0.4840\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4690 - acc: 0.5159 - val_loss: 1.4775 - val_acc: 0.5030\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4198 - acc: 0.5348 - val_loss: 1.4305 - val_acc: 0.5240\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3708 - acc: 0.5641 - val_loss: 1.3836 - val_acc: 0.5460\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3223 - acc: 0.5864 - val_loss: 1.3371 - val_acc: 0.5670\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2742 - acc: 0.6064 - val_loss: 1.2934 - val_acc: 0.5840\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2281 - acc: 0.6252 - val_loss: 1.2504 - val_acc: 0.6090\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1835 - acc: 0.6411 - val_loss: 1.2098 - val_acc: 0.6270\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1411 - acc: 0.6544 - val_loss: 1.1706 - val_acc: 0.6310\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1007 - acc: 0.6659 - val_loss: 1.1356 - val_acc: 0.6430\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0634 - acc: 0.6775 - val_loss: 1.0987 - val_acc: 0.6540\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0276 - acc: 0.6865 - val_loss: 1.0687 - val_acc: 0.6590\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9945 - acc: 0.6943 - val_loss: 1.0364 - val_acc: 0.6810\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9634 - acc: 0.7033 - val_loss: 1.0086 - val_acc: 0.6860\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9349 - acc: 0.7104 - val_loss: 0.9840 - val_acc: 0.6930\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9085 - acc: 0.7152 - val_loss: 0.9575 - val_acc: 0.6970\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8839 - acc: 0.7233 - val_loss: 0.9387 - val_acc: 0.7090\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8607 - acc: 0.7277 - val_loss: 0.9221 - val_acc: 0.7030\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8396 - acc: 0.7315 - val_loss: 0.8978 - val_acc: 0.7080\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8195 - acc: 0.7392 - val_loss: 0.8801 - val_acc: 0.7130\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8013 - acc: 0.7408 - val_loss: 0.8628 - val_acc: 0.7190\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7839 - acc: 0.7471 - val_loss: 0.8484 - val_acc: 0.7280\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7679 - acc: 0.7512 - val_loss: 0.8349 - val_acc: 0.7270\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7530 - acc: 0.7555 - val_loss: 0.8222 - val_acc: 0.7320\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7392 - acc: 0.7575 - val_loss: 0.8109 - val_acc: 0.7320\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7260 - acc: 0.7607 - val_loss: 0.7995 - val_acc: 0.7340\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7139 - acc: 0.7653 - val_loss: 0.7896 - val_acc: 0.7370\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7016 - acc: 0.7687 - val_loss: 0.7826 - val_acc: 0.7320\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6905 - acc: 0.7725 - val_loss: 0.7742 - val_acc: 0.7310\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6803 - acc: 0.7739 - val_loss: 0.7645 - val_acc: 0.7350\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6704 - acc: 0.7741 - val_loss: 0.7549 - val_acc: 0.7450\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6608 - acc: 0.7788 - val_loss: 0.7488 - val_acc: 0.7430\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.6513 - acc: 0.7804 - val_loss: 0.7426 - val_acc: 0.7450\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6429 - acc: 0.7835 - val_loss: 0.7360 - val_acc: 0.7490\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6344 - acc: 0.7859 - val_loss: 0.7306 - val_acc: 0.7470\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6268 - acc: 0.7897 - val_loss: 0.7228 - val_acc: 0.7480\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.6195 - acc: 0.7911 - val_loss: 0.7174 - val_acc: 0.7520\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6117 - acc: 0.7941 - val_loss: 0.7173 - val_acc: 0.7500\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6047 - acc: 0.7940 - val_loss: 0.7103 - val_acc: 0.7490\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5980 - acc: 0.7973 - val_loss: 0.7060 - val_acc: 0.7510\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.5912 - acc: 0.7988 - val_loss: 0.7037 - val_acc: 0.7510\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5846 - acc: 0.8015 - val_loss: 0.6974 - val_acc: 0.7540\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5792 - acc: 0.8017 - val_loss: 0.6923 - val_acc: 0.7550\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.5730 - acc: 0.8029 - val_loss: 0.6883 - val_acc: 0.7540\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5671 - acc: 0.8064 - val_loss: 0.6865 - val_acc: 0.7530\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.5611 - acc: 0.8060 - val_loss: 0.6810 - val_acc: 0.7530\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5555 - acc: 0.8108 - val_loss: 0.6792 - val_acc: 0.7660\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5503 - acc: 0.8121 - val_loss: 0.6759 - val_acc: 0.7620\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5447 - acc: 0.8112 - val_loss: 0.6727 - val_acc: 0.7650\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5401 - acc: 0.8151 - val_loss: 0.6697 - val_acc: 0.7600\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5352 - acc: 0.8167 - val_loss: 0.6660 - val_acc: 0.7610\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5303 - acc: 0.8161 - val_loss: 0.6666 - val_acc: 0.7590\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5253 - acc: 0.8192 - val_loss: 0.6630 - val_acc: 0.7670\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5202 - acc: 0.8203 - val_loss: 0.6657 - val_acc: 0.7590\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5160 - acc: 0.8200 - val_loss: 0.6591 - val_acc: 0.7630\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5115 - acc: 0.8243 - val_loss: 0.6606 - val_acc: 0.7620\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5071 - acc: 0.8247 - val_loss: 0.6554 - val_acc: 0.7660\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5026 - acc: 0.8284 - val_loss: 0.6523 - val_acc: 0.7710\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4984 - acc: 0.8267 - val_loss: 0.6516 - val_acc: 0.7660\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4942 - acc: 0.8304 - val_loss: 0.6539 - val_acc: 0.7650\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.4900 - acc: 0.8345 - val_loss: 0.6481 - val_acc: 0.7700\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4859 - acc: 0.8328 - val_loss: 0.6502 - val_acc: 0.7670\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.4822 - acc: 0.8357 - val_loss: 0.6454 - val_acc: 0.7730\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4782 - acc: 0.8357 - val_loss: 0.6420 - val_acc: 0.7690\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.4745 - acc: 0.8384 - val_loss: 0.6455 - val_acc: 0.7680\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4709 - acc: 0.8405 - val_loss: 0.6406 - val_acc: 0.7700\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4669 - acc: 0.8409 - val_loss: 0.6404 - val_acc: 0.7690\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4632 - acc: 0.8429 - val_loss: 0.6380 - val_acc: 0.7740\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4600 - acc: 0.8455 - val_loss: 0.6343 - val_acc: 0.7740\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.4562 - acc: 0.8448 - val_loss: 0.6384 - val_acc: 0.7700\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4525 - acc: 0.8493 - val_loss: 0.6379 - val_acc: 0.7720\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4491 - acc: 0.8488 - val_loss: 0.6325 - val_acc: 0.7700\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4461 - acc: 0.8503 - val_loss: 0.6360 - val_acc: 0.7640\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.4427 - acc: 0.8525 - val_loss: 0.6313 - val_acc: 0.7730\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4390 - acc: 0.8523 - val_loss: 0.6335 - val_acc: 0.7730\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4361 - acc: 0.8535 - val_loss: 0.6301 - val_acc: 0.7730\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.4328 - acc: 0.8545 - val_loss: 0.6313 - val_acc: 0.7700\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4296 - acc: 0.8560 - val_loss: 0.6305 - val_acc: 0.7690\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4265 - acc: 0.8575 - val_loss: 0.6260 - val_acc: 0.7700\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4231 - acc: 0.8585 - val_loss: 0.6332 - val_acc: 0.7700\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4199 - acc: 0.8596 - val_loss: 0.6258 - val_acc: 0.7760\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4168 - acc: 0.8616 - val_loss: 0.6245 - val_acc: 0.7640\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.4145 - acc: 0.8636 - val_loss: 0.6267 - val_acc: 0.7640\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4113 - acc: 0.8624 - val_loss: 0.6297 - val_acc: 0.7680\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4083 - acc: 0.8651 - val_loss: 0.6236 - val_acc: 0.7690\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4051 - acc: 0.8673 - val_loss: 0.6224 - val_acc: 0.7740\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4023 - acc: 0.8667 - val_loss: 0.6284 - val_acc: 0.7760\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3995 - acc: 0.8691 - val_loss: 0.6217 - val_acc: 0.7720\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3968 - acc: 0.8688 - val_loss: 0.6195 - val_acc: 0.7680\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3936 - acc: 0.8708 - val_loss: 0.6224 - val_acc: 0.7730\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3911 - acc: 0.8709 - val_loss: 0.6227 - val_acc: 0.7660\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3885 - acc: 0.8713 - val_loss: 0.6225 - val_acc: 0.7660\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3855 - acc: 0.8728 - val_loss: 0.6201 - val_acc: 0.7710\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3826 - acc: 0.8763 - val_loss: 0.6192 - val_acc: 0.7660\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3805 - acc: 0.8748 - val_loss: 0.6198 - val_acc: 0.7680\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.3776 - acc: 0.8783 - val_loss: 0.6191 - val_acc: 0.7620\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3749 - acc: 0.8765 - val_loss: 0.6202 - val_acc: 0.7620\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3718 - acc: 0.8785 - val_loss: 0.6230 - val_acc: 0.7660\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3699 - acc: 0.8785 - val_loss: 0.6194 - val_acc: 0.7680\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3669 - acc: 0.8785 - val_loss: 0.6184 - val_acc: 0.7650\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3645 - acc: 0.8811 - val_loss: 0.6177 - val_acc: 0.7720\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3621 - acc: 0.8815 - val_loss: 0.6228 - val_acc: 0.7640\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3603 - acc: 0.8812 - val_loss: 0.6179 - val_acc: 0.7690\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3575 - acc: 0.8837 - val_loss: 0.6177 - val_acc: 0.7690\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.3545 - acc: 0.8840 - val_loss: 0.6203 - val_acc: 0.7670\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3522 - acc: 0.8867 - val_loss: 0.6179 - val_acc: 0.7710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3503 - acc: 0.8868 - val_loss: 0.6185 - val_acc: 0.7750\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 41us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3465754810969035, 0.8884]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.612536981980006, 0.7713333334922791]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9284 - acc: 0.1745 - val_loss: 1.9233 - val_acc: 0.1750\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9081 - acc: 0.2004 - val_loss: 1.9057 - val_acc: 0.1930\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8895 - acc: 0.2224 - val_loss: 1.8886 - val_acc: 0.2020\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8704 - acc: 0.2487 - val_loss: 1.8705 - val_acc: 0.2310\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8494 - acc: 0.2780 - val_loss: 1.8503 - val_acc: 0.2560\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8251 - acc: 0.3100 - val_loss: 1.8267 - val_acc: 0.2990\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7972 - acc: 0.3424 - val_loss: 1.7996 - val_acc: 0.3260\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7661 - acc: 0.3721 - val_loss: 1.7697 - val_acc: 0.3580\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7318 - acc: 0.4057 - val_loss: 1.7366 - val_acc: 0.3760\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6952 - acc: 0.4305 - val_loss: 1.7015 - val_acc: 0.4230\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6562 - acc: 0.4616 - val_loss: 1.6641 - val_acc: 0.4380\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6151 - acc: 0.4841 - val_loss: 1.6254 - val_acc: 0.4640\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5719 - acc: 0.5015 - val_loss: 1.5846 - val_acc: 0.4950\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5272 - acc: 0.5277 - val_loss: 1.5426 - val_acc: 0.5130\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4807 - acc: 0.5476 - val_loss: 1.4988 - val_acc: 0.5450\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4333 - acc: 0.5665 - val_loss: 1.4546 - val_acc: 0.5550\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3850 - acc: 0.5875 - val_loss: 1.4091 - val_acc: 0.5770\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3368 - acc: 0.6044 - val_loss: 1.3641 - val_acc: 0.5940\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2890 - acc: 0.6224 - val_loss: 1.3205 - val_acc: 0.6110\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2426 - acc: 0.6393 - val_loss: 1.2769 - val_acc: 0.6310\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1978 - acc: 0.6524 - val_loss: 1.2363 - val_acc: 0.6440\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1547 - acc: 0.6664 - val_loss: 1.1962 - val_acc: 0.6550\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1141 - acc: 0.6771 - val_loss: 1.1600 - val_acc: 0.6660\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0752 - acc: 0.6841 - val_loss: 1.1235 - val_acc: 0.6700\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0388 - acc: 0.6928 - val_loss: 1.0903 - val_acc: 0.6720\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0042 - acc: 0.6995 - val_loss: 1.0590 - val_acc: 0.6800\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9726 - acc: 0.7077 - val_loss: 1.0308 - val_acc: 0.6840\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9427 - acc: 0.7152 - val_loss: 1.0036 - val_acc: 0.6870\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9148 - acc: 0.7195 - val_loss: 0.9810 - val_acc: 0.6920\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8892 - acc: 0.7249 - val_loss: 0.9565 - val_acc: 0.6990\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8654 - acc: 0.7292 - val_loss: 0.9339 - val_acc: 0.6960\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8433 - acc: 0.7319 - val_loss: 0.9164 - val_acc: 0.6900\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8228 - acc: 0.7376 - val_loss: 0.8971 - val_acc: 0.7000\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8039 - acc: 0.7428 - val_loss: 0.8796 - val_acc: 0.7130\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7860 - acc: 0.7449 - val_loss: 0.8641 - val_acc: 0.7100\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.7700 - acc: 0.7469 - val_loss: 0.8511 - val_acc: 0.7170\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7543 - acc: 0.7495 - val_loss: 0.8378 - val_acc: 0.7180\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7402 - acc: 0.7523 - val_loss: 0.8265 - val_acc: 0.7170\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7268 - acc: 0.7595 - val_loss: 0.8151 - val_acc: 0.7270\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7144 - acc: 0.7629 - val_loss: 0.8050 - val_acc: 0.7280\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7024 - acc: 0.7640 - val_loss: 0.7953 - val_acc: 0.7280\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6916 - acc: 0.7685 - val_loss: 0.7865 - val_acc: 0.7370\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.6810 - acc: 0.7707 - val_loss: 0.7796 - val_acc: 0.7350\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6714 - acc: 0.7700 - val_loss: 0.7724 - val_acc: 0.7300\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6619 - acc: 0.7745 - val_loss: 0.7638 - val_acc: 0.7310\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.6535 - acc: 0.7771 - val_loss: 0.7636 - val_acc: 0.7350\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6447 - acc: 0.7789 - val_loss: 0.7533 - val_acc: 0.7360\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6365 - acc: 0.7840 - val_loss: 0.7456 - val_acc: 0.7410\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6287 - acc: 0.7832 - val_loss: 0.7410 - val_acc: 0.7420\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6216 - acc: 0.7893 - val_loss: 0.7363 - val_acc: 0.7390\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.6145 - acc: 0.7881 - val_loss: 0.7337 - val_acc: 0.7420\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6075 - acc: 0.7929 - val_loss: 0.7279 - val_acc: 0.7460\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6008 - acc: 0.7929 - val_loss: 0.7218 - val_acc: 0.7410\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5945 - acc: 0.7975 - val_loss: 0.7173 - val_acc: 0.7430\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5884 - acc: 0.7992 - val_loss: 0.7127 - val_acc: 0.7410\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5822 - acc: 0.8011 - val_loss: 0.7103 - val_acc: 0.7400\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5763 - acc: 0.8056 - val_loss: 0.7068 - val_acc: 0.7450\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5712 - acc: 0.8037 - val_loss: 0.7034 - val_acc: 0.7440\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.5648 - acc: 0.8064 - val_loss: 0.7049 - val_acc: 0.7460\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5601 - acc: 0.8076 - val_loss: 0.6984 - val_acc: 0.7430\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 32us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 41us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5552897730191548, 0.8101333333015441]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6573357983430227, 0.7460000001589457]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 2.6041 - acc: 0.1632 - val_loss: 2.5851 - val_acc: 0.1840\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.5742 - acc: 0.1923 - val_loss: 2.5636 - val_acc: 0.2260\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.5547 - acc: 0.2271 - val_loss: 2.5461 - val_acc: 0.2460\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5368 - acc: 0.2455 - val_loss: 2.5289 - val_acc: 0.2630\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.5184 - acc: 0.2648 - val_loss: 2.5107 - val_acc: 0.2780\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.4986 - acc: 0.2840 - val_loss: 2.4910 - val_acc: 0.3000\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.4767 - acc: 0.3036 - val_loss: 2.4691 - val_acc: 0.3250\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.4522 - acc: 0.3175 - val_loss: 2.4444 - val_acc: 0.3400\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.4245 - acc: 0.3397 - val_loss: 2.4164 - val_acc: 0.3520\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.3932 - acc: 0.3607 - val_loss: 2.3851 - val_acc: 0.3700\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.3581 - acc: 0.3819 - val_loss: 2.3494 - val_acc: 0.3940\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.3187 - acc: 0.4036 - val_loss: 2.3100 - val_acc: 0.4130\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.2759 - acc: 0.4393 - val_loss: 2.2670 - val_acc: 0.4390\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.2293 - acc: 0.4597 - val_loss: 2.2226 - val_acc: 0.4670\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.1805 - acc: 0.4932 - val_loss: 2.1741 - val_acc: 0.4940\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.1297 - acc: 0.5184 - val_loss: 2.1255 - val_acc: 0.5230\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.0782 - acc: 0.5404 - val_loss: 2.0761 - val_acc: 0.5420\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.0268 - acc: 0.5615 - val_loss: 2.0273 - val_acc: 0.5710\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9762 - acc: 0.5840 - val_loss: 1.9798 - val_acc: 0.5940\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9266 - acc: 0.6048 - val_loss: 1.9330 - val_acc: 0.6180\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8779 - acc: 0.6252 - val_loss: 1.8869 - val_acc: 0.6410\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8303 - acc: 0.6459 - val_loss: 1.8424 - val_acc: 0.6420\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7851 - acc: 0.6576 - val_loss: 1.7991 - val_acc: 0.6580\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7410 - acc: 0.6712 - val_loss: 1.7577 - val_acc: 0.6750\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6988 - acc: 0.6832 - val_loss: 1.7199 - val_acc: 0.6870\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6592 - acc: 0.6955 - val_loss: 1.6832 - val_acc: 0.6890\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6217 - acc: 0.7028 - val_loss: 1.6477 - val_acc: 0.6900\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5865 - acc: 0.7100 - val_loss: 1.6148 - val_acc: 0.6950\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5543 - acc: 0.7164 - val_loss: 1.5877 - val_acc: 0.7060\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5241 - acc: 0.7227 - val_loss: 1.5583 - val_acc: 0.7090\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4967 - acc: 0.7244 - val_loss: 1.5346 - val_acc: 0.7150\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4709 - acc: 0.7317 - val_loss: 1.5139 - val_acc: 0.7190\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4470 - acc: 0.7365 - val_loss: 1.4910 - val_acc: 0.7210\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4251 - acc: 0.7391 - val_loss: 1.4710 - val_acc: 0.7130\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4046 - acc: 0.7435 - val_loss: 1.4499 - val_acc: 0.7240\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3852 - acc: 0.7455 - val_loss: 1.4349 - val_acc: 0.7260\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3675 - acc: 0.7484 - val_loss: 1.4172 - val_acc: 0.7230\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3509 - acc: 0.7515 - val_loss: 1.4040 - val_acc: 0.7270\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3352 - acc: 0.7552 - val_loss: 1.3989 - val_acc: 0.7300\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3209 - acc: 0.7585 - val_loss: 1.3759 - val_acc: 0.7260\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3066 - acc: 0.7599 - val_loss: 1.3648 - val_acc: 0.7260\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2932 - acc: 0.7652 - val_loss: 1.3538 - val_acc: 0.7290\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2807 - acc: 0.7665 - val_loss: 1.3448 - val_acc: 0.7350\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2688 - acc: 0.7675 - val_loss: 1.3355 - val_acc: 0.7340\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2580 - acc: 0.7661 - val_loss: 1.3258 - val_acc: 0.7380\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2474 - acc: 0.7724 - val_loss: 1.3166 - val_acc: 0.7380\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2370 - acc: 0.7716 - val_loss: 1.3055 - val_acc: 0.7380\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2267 - acc: 0.7761 - val_loss: 1.2998 - val_acc: 0.7380\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2173 - acc: 0.7759 - val_loss: 1.2891 - val_acc: 0.7310\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2081 - acc: 0.7775 - val_loss: 1.2829 - val_acc: 0.7400\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1993 - acc: 0.7796 - val_loss: 1.2761 - val_acc: 0.7380\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1905 - acc: 0.7797 - val_loss: 1.2719 - val_acc: 0.7410\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1823 - acc: 0.7873 - val_loss: 1.2663 - val_acc: 0.7410\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1746 - acc: 0.7845 - val_loss: 1.2564 - val_acc: 0.7390\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1664 - acc: 0.7879 - val_loss: 1.2494 - val_acc: 0.7380\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1590 - acc: 0.7895 - val_loss: 1.2452 - val_acc: 0.7440\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1518 - acc: 0.7900 - val_loss: 1.2409 - val_acc: 0.7420\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1449 - acc: 0.7947 - val_loss: 1.2346 - val_acc: 0.7420\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1374 - acc: 0.7952 - val_loss: 1.2302 - val_acc: 0.7420\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1307 - acc: 0.7952 - val_loss: 1.2228 - val_acc: 0.7440\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1236 - acc: 0.7996 - val_loss: 1.2230 - val_acc: 0.7460\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1172 - acc: 0.7993 - val_loss: 1.2118 - val_acc: 0.7430\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1108 - acc: 0.8016 - val_loss: 1.2130 - val_acc: 0.7450\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1046 - acc: 0.8012 - val_loss: 1.2090 - val_acc: 0.7490\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0985 - acc: 0.8049 - val_loss: 1.2000 - val_acc: 0.7500\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0918 - acc: 0.8040 - val_loss: 1.1990 - val_acc: 0.7520\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0863 - acc: 0.8073 - val_loss: 1.1907 - val_acc: 0.7480\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0805 - acc: 0.8091 - val_loss: 1.1885 - val_acc: 0.7470\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0747 - acc: 0.8087 - val_loss: 1.1828 - val_acc: 0.7480\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0689 - acc: 0.8113 - val_loss: 1.1779 - val_acc: 0.7520\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0632 - acc: 0.8132 - val_loss: 1.1760 - val_acc: 0.7530\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0577 - acc: 0.8168 - val_loss: 1.1724 - val_acc: 0.7500\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0523 - acc: 0.8135 - val_loss: 1.1649 - val_acc: 0.7530\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0472 - acc: 0.8153 - val_loss: 1.1631 - val_acc: 0.7510\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0414 - acc: 0.8229 - val_loss: 1.1583 - val_acc: 0.7490\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0364 - acc: 0.8219 - val_loss: 1.1577 - val_acc: 0.7520\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0315 - acc: 0.8229 - val_loss: 1.1561 - val_acc: 0.7530\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0266 - acc: 0.8239 - val_loss: 1.1496 - val_acc: 0.7510\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0212 - acc: 0.8223 - val_loss: 1.1461 - val_acc: 0.7510\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0165 - acc: 0.8267 - val_loss: 1.1423 - val_acc: 0.7510\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0115 - acc: 0.8279 - val_loss: 1.1404 - val_acc: 0.7560\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0066 - acc: 0.8265 - val_loss: 1.1348 - val_acc: 0.7560\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0016 - acc: 0.8320 - val_loss: 1.1345 - val_acc: 0.7550\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9972 - acc: 0.8297 - val_loss: 1.1307 - val_acc: 0.7560\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9923 - acc: 0.8320 - val_loss: 1.1263 - val_acc: 0.7590\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9880 - acc: 0.8328 - val_loss: 1.1230 - val_acc: 0.7570\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9831 - acc: 0.8349 - val_loss: 1.1211 - val_acc: 0.7600\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9788 - acc: 0.8347 - val_loss: 1.1198 - val_acc: 0.7580\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9743 - acc: 0.8377 - val_loss: 1.1161 - val_acc: 0.7600\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9702 - acc: 0.8385 - val_loss: 1.1129 - val_acc: 0.7590\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9657 - acc: 0.8401 - val_loss: 1.1094 - val_acc: 0.7600\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9614 - acc: 0.8408 - val_loss: 1.1054 - val_acc: 0.7600\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9569 - acc: 0.8441 - val_loss: 1.1032 - val_acc: 0.7600\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9528 - acc: 0.8436 - val_loss: 1.1037 - val_acc: 0.7630\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9485 - acc: 0.8452 - val_loss: 1.0986 - val_acc: 0.7660\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9443 - acc: 0.8472 - val_loss: 1.0941 - val_acc: 0.7640\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9402 - acc: 0.8491 - val_loss: 1.0939 - val_acc: 0.7640\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9359 - acc: 0.8495 - val_loss: 1.0919 - val_acc: 0.7620\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9318 - acc: 0.8492 - val_loss: 1.0903 - val_acc: 0.7650\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9280 - acc: 0.8504 - val_loss: 1.0886 - val_acc: 0.7630\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9239 - acc: 0.8507 - val_loss: 1.0831 - val_acc: 0.7690\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9200 - acc: 0.8540 - val_loss: 1.0801 - val_acc: 0.7670\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9161 - acc: 0.8532 - val_loss: 1.0790 - val_acc: 0.7660\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9124 - acc: 0.8541 - val_loss: 1.0759 - val_acc: 0.7690\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9085 - acc: 0.8569 - val_loss: 1.0769 - val_acc: 0.7670\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9048 - acc: 0.8555 - val_loss: 1.0712 - val_acc: 0.7750\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9009 - acc: 0.8581 - val_loss: 1.0711 - val_acc: 0.7700\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8971 - acc: 0.8569 - val_loss: 1.0714 - val_acc: 0.7640\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8937 - acc: 0.8581 - val_loss: 1.0676 - val_acc: 0.7670\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8901 - acc: 0.8589 - val_loss: 1.0625 - val_acc: 0.7720\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8864 - acc: 0.8607 - val_loss: 1.0594 - val_acc: 0.7700\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8826 - acc: 0.8623 - val_loss: 1.0628 - val_acc: 0.7700\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8790 - acc: 0.8609 - val_loss: 1.0564 - val_acc: 0.7720\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8753 - acc: 0.8643 - val_loss: 1.0535 - val_acc: 0.7710\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8721 - acc: 0.8653 - val_loss: 1.0512 - val_acc: 0.7720\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8684 - acc: 0.8648 - val_loss: 1.0485 - val_acc: 0.7730\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8650 - acc: 0.8661 - val_loss: 1.0470 - val_acc: 0.7770\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8612 - acc: 0.8675 - val_loss: 1.0490 - val_acc: 0.7680\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8585 - acc: 0.8672 - val_loss: 1.0445 - val_acc: 0.7720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8545 - acc: 0.8671 - val_loss: 1.0449 - val_acc: 0.7670\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 16.0270 - acc: 0.1701 - val_loss: 15.6240 - val_acc: 0.1920\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 15.2701 - acc: 0.1855 - val_loss: 14.8808 - val_acc: 0.2060\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 14.5362 - acc: 0.1951 - val_loss: 14.1598 - val_acc: 0.2100\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 13.8233 - acc: 0.2044 - val_loss: 13.4588 - val_acc: 0.2230\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 13.1293 - acc: 0.2131 - val_loss: 12.7758 - val_acc: 0.2410\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 12.4529 - acc: 0.2305 - val_loss: 12.1106 - val_acc: 0.2540\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 11.7950 - acc: 0.2592 - val_loss: 11.4643 - val_acc: 0.2810\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 11.1562 - acc: 0.2813 - val_loss: 10.8383 - val_acc: 0.3070\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 10.5373 - acc: 0.3136 - val_loss: 10.2314 - val_acc: 0.3310\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 9.9380 - acc: 0.3437 - val_loss: 9.6442 - val_acc: 0.3610\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 9.3585 - acc: 0.3708 - val_loss: 9.0783 - val_acc: 0.3890\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 8.7999 - acc: 0.4097 - val_loss: 8.5311 - val_acc: 0.4240\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 8.2620 - acc: 0.4403 - val_loss: 8.0065 - val_acc: 0.4420\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 7.7457 - acc: 0.4655 - val_loss: 7.5041 - val_acc: 0.4640\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 7.2522 - acc: 0.4961 - val_loss: 7.0241 - val_acc: 0.5010\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 6.7826 - acc: 0.5216 - val_loss: 6.5675 - val_acc: 0.5180\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 6.3359 - acc: 0.5465 - val_loss: 6.1335 - val_acc: 0.5380\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 5.9117 - acc: 0.5644 - val_loss: 5.7215 - val_acc: 0.5550\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 5.5108 - acc: 0.5817 - val_loss: 5.3342 - val_acc: 0.5870\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 5.1331 - acc: 0.5991 - val_loss: 4.9700 - val_acc: 0.6090\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 4.7789 - acc: 0.6167 - val_loss: 4.6277 - val_acc: 0.6210\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 4.4481 - acc: 0.6281 - val_loss: 4.3091 - val_acc: 0.6370\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 4.1401 - acc: 0.6389 - val_loss: 4.0134 - val_acc: 0.6470\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 3.8541 - acc: 0.6479 - val_loss: 3.7390 - val_acc: 0.6460\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 3.5900 - acc: 0.6540 - val_loss: 3.4870 - val_acc: 0.6620\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 3.3492 - acc: 0.6615 - val_loss: 3.2566 - val_acc: 0.6670\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 3.1299 - acc: 0.6641 - val_loss: 3.0482 - val_acc: 0.6720\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.9328 - acc: 0.6664 - val_loss: 2.8640 - val_acc: 0.6740\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.7576 - acc: 0.6753 - val_loss: 2.6996 - val_acc: 0.6730\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.6042 - acc: 0.6741 - val_loss: 2.5554 - val_acc: 0.6770\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.4713 - acc: 0.6757 - val_loss: 2.4337 - val_acc: 0.6780\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.3594 - acc: 0.6783 - val_loss: 2.3317 - val_acc: 0.6800\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.2669 - acc: 0.6780 - val_loss: 2.2477 - val_acc: 0.6900\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.1924 - acc: 0.6831 - val_loss: 2.1830 - val_acc: 0.6870\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.1352 - acc: 0.6841 - val_loss: 2.1329 - val_acc: 0.6810\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.0926 - acc: 0.6847 - val_loss: 2.0968 - val_acc: 0.6830\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.0608 - acc: 0.6863 - val_loss: 2.0681 - val_acc: 0.6820\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.0348 - acc: 0.6869 - val_loss: 2.0454 - val_acc: 0.6860\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.0126 - acc: 0.6848 - val_loss: 2.0240 - val_acc: 0.6800\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9920 - acc: 0.6879 - val_loss: 2.0039 - val_acc: 0.6890\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9729 - acc: 0.6884 - val_loss: 1.9860 - val_acc: 0.6960\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9551 - acc: 0.6899 - val_loss: 1.9683 - val_acc: 0.6930\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9377 - acc: 0.6923 - val_loss: 1.9506 - val_acc: 0.6940\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9215 - acc: 0.6927 - val_loss: 1.9355 - val_acc: 0.6890\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9056 - acc: 0.6955 - val_loss: 1.9220 - val_acc: 0.6850\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8903 - acc: 0.6956 - val_loss: 1.9055 - val_acc: 0.6920\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8754 - acc: 0.6957 - val_loss: 1.8916 - val_acc: 0.6970\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8610 - acc: 0.6968 - val_loss: 1.8779 - val_acc: 0.6940\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8469 - acc: 0.6995 - val_loss: 1.8637 - val_acc: 0.6930\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8333 - acc: 0.6997 - val_loss: 1.8547 - val_acc: 0.6970\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8199 - acc: 0.6983 - val_loss: 1.8381 - val_acc: 0.6980\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8069 - acc: 0.6995 - val_loss: 1.8284 - val_acc: 0.6900\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7947 - acc: 0.6997 - val_loss: 1.8126 - val_acc: 0.6980\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7825 - acc: 0.7007 - val_loss: 1.8006 - val_acc: 0.6980\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7705 - acc: 0.7005 - val_loss: 1.7933 - val_acc: 0.6900\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7592 - acc: 0.7025 - val_loss: 1.7774 - val_acc: 0.7010\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7477 - acc: 0.7025 - val_loss: 1.7660 - val_acc: 0.7060\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7368 - acc: 0.7028 - val_loss: 1.7564 - val_acc: 0.7010\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7261 - acc: 0.7044 - val_loss: 1.7448 - val_acc: 0.7020\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7153 - acc: 0.7051 - val_loss: 1.7379 - val_acc: 0.7010\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7056 - acc: 0.7047 - val_loss: 1.7257 - val_acc: 0.7020\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6951 - acc: 0.7051 - val_loss: 1.7169 - val_acc: 0.7060\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6852 - acc: 0.7071 - val_loss: 1.7060 - val_acc: 0.7080\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6753 - acc: 0.7068 - val_loss: 1.6966 - val_acc: 0.7050\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6661 - acc: 0.7065 - val_loss: 1.6910 - val_acc: 0.7060\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6567 - acc: 0.7068 - val_loss: 1.6807 - val_acc: 0.7090\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6474 - acc: 0.7069 - val_loss: 1.6694 - val_acc: 0.7100\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6384 - acc: 0.7084 - val_loss: 1.6627 - val_acc: 0.7080\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6302 - acc: 0.7075 - val_loss: 1.6539 - val_acc: 0.7070\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6214 - acc: 0.7077 - val_loss: 1.6458 - val_acc: 0.7060\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6123 - acc: 0.7083 - val_loss: 1.6353 - val_acc: 0.7060\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6046 - acc: 0.7095 - val_loss: 1.6305 - val_acc: 0.7090\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5959 - acc: 0.7097 - val_loss: 1.6210 - val_acc: 0.7120\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5877 - acc: 0.7103 - val_loss: 1.6127 - val_acc: 0.7040\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5797 - acc: 0.7111 - val_loss: 1.6059 - val_acc: 0.7060\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5713 - acc: 0.7124 - val_loss: 1.5985 - val_acc: 0.7040\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.5639 - acc: 0.7123 - val_loss: 1.5901 - val_acc: 0.7080\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5564 - acc: 0.7132 - val_loss: 1.5826 - val_acc: 0.7110\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5488 - acc: 0.7113 - val_loss: 1.5735 - val_acc: 0.7100\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5408 - acc: 0.7148 - val_loss: 1.5676 - val_acc: 0.7100\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5338 - acc: 0.7144 - val_loss: 1.5611 - val_acc: 0.7080\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.5268 - acc: 0.7152 - val_loss: 1.5541 - val_acc: 0.7070\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5193 - acc: 0.7152 - val_loss: 1.5464 - val_acc: 0.7150\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5124 - acc: 0.7165 - val_loss: 1.5418 - val_acc: 0.7090\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5059 - acc: 0.7153 - val_loss: 1.5329 - val_acc: 0.7120\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4991 - acc: 0.7156 - val_loss: 1.5272 - val_acc: 0.7140\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4920 - acc: 0.7175 - val_loss: 1.5208 - val_acc: 0.7150\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4854 - acc: 0.7184 - val_loss: 1.5138 - val_acc: 0.7100\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4787 - acc: 0.7183 - val_loss: 1.5130 - val_acc: 0.7110\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4729 - acc: 0.7196 - val_loss: 1.5017 - val_acc: 0.7150\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4664 - acc: 0.7179 - val_loss: 1.4961 - val_acc: 0.7160\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4603 - acc: 0.7189 - val_loss: 1.4913 - val_acc: 0.7150\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4539 - acc: 0.7193 - val_loss: 1.4846 - val_acc: 0.7170\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4473 - acc: 0.7193 - val_loss: 1.4795 - val_acc: 0.7150\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4418 - acc: 0.7193 - val_loss: 1.4721 - val_acc: 0.7150\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4357 - acc: 0.7211 - val_loss: 1.4675 - val_acc: 0.7130\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4294 - acc: 0.7212 - val_loss: 1.4618 - val_acc: 0.7140\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4237 - acc: 0.7204 - val_loss: 1.4555 - val_acc: 0.7200\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4179 - acc: 0.7220 - val_loss: 1.4525 - val_acc: 0.7130\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4124 - acc: 0.7237 - val_loss: 1.4470 - val_acc: 0.7150\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4069 - acc: 0.7213 - val_loss: 1.4370 - val_acc: 0.7190\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4011 - acc: 0.7227 - val_loss: 1.4335 - val_acc: 0.7190\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3955 - acc: 0.7231 - val_loss: 1.4279 - val_acc: 0.7190\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3905 - acc: 0.7223 - val_loss: 1.4215 - val_acc: 0.7160\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3845 - acc: 0.7229 - val_loss: 1.4203 - val_acc: 0.7120\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3793 - acc: 0.7245 - val_loss: 1.4125 - val_acc: 0.7210\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3743 - acc: 0.7236 - val_loss: 1.4081 - val_acc: 0.7180\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3688 - acc: 0.7232 - val_loss: 1.4030 - val_acc: 0.7180\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3633 - acc: 0.7235 - val_loss: 1.3993 - val_acc: 0.7170\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3579 - acc: 0.7255 - val_loss: 1.3937 - val_acc: 0.7200\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3530 - acc: 0.7263 - val_loss: 1.3885 - val_acc: 0.7200\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3484 - acc: 0.7249 - val_loss: 1.3822 - val_acc: 0.7180\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3430 - acc: 0.7247 - val_loss: 1.3804 - val_acc: 0.7200\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3382 - acc: 0.7252 - val_loss: 1.3749 - val_acc: 0.7210\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3338 - acc: 0.7256 - val_loss: 1.3729 - val_acc: 0.7200\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3295 - acc: 0.7259 - val_loss: 1.3660 - val_acc: 0.7220\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3241 - acc: 0.7257 - val_loss: 1.3620 - val_acc: 0.7200\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3194 - acc: 0.7255 - val_loss: 1.3553 - val_acc: 0.7190\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3151 - acc: 0.7260 - val_loss: 1.3521 - val_acc: 0.7190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3106 - acc: 0.7265 - val_loss: 1.3490 - val_acc: 0.7200\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L1_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d9f7a74ada55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mL1_model_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL1_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0macc_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL1_model_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_acc_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL1_model_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L1_model' is not defined"
     ]
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 16.0140 - acc: 0.1877 - val_loss: 15.6118 - val_acc: 0.2100\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 15.2533 - acc: 0.2067 - val_loss: 14.8668 - val_acc: 0.2240\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 14.5171 - acc: 0.2219 - val_loss: 14.1433 - val_acc: 0.2290\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 13.8011 - acc: 0.2360 - val_loss: 13.4388 - val_acc: 0.2440\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 13.1038 - acc: 0.2528 - val_loss: 12.7534 - val_acc: 0.2580\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 12.4253 - acc: 0.2772 - val_loss: 12.0869 - val_acc: 0.2720\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 11.7658 - acc: 0.3027 - val_loss: 11.4387 - val_acc: 0.3140\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 11.1259 - acc: 0.3431 - val_loss: 10.8103 - val_acc: 0.3470\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 10.5062 - acc: 0.3781 - val_loss: 10.2028 - val_acc: 0.3780\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.9068 - acc: 0.4124 - val_loss: 9.6137 - val_acc: 0.3970\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.3279 - acc: 0.4441 - val_loss: 9.0463 - val_acc: 0.4310\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 8.7708 - acc: 0.4668 - val_loss: 8.5007 - val_acc: 0.4610\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 8.2357 - acc: 0.4971 - val_loss: 7.9774 - val_acc: 0.4730\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 7.7237 - acc: 0.5213 - val_loss: 7.4776 - val_acc: 0.4990\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 7.2345 - acc: 0.5405 - val_loss: 7.0011 - val_acc: 0.5270\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 6.7686 - acc: 0.5620 - val_loss: 6.5474 - val_acc: 0.5490\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 6.3256 - acc: 0.5787 - val_loss: 6.1182 - val_acc: 0.5860\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.9053 - acc: 0.5941 - val_loss: 5.7067 - val_acc: 0.5940\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 5.5074 - acc: 0.6119 - val_loss: 5.3218 - val_acc: 0.5940\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 5.1324 - acc: 0.6183 - val_loss: 4.9565 - val_acc: 0.6200\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 4.7803 - acc: 0.6311 - val_loss: 4.6160 - val_acc: 0.6160\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 4.4521 - acc: 0.6364 - val_loss: 4.2978 - val_acc: 0.6320\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 4.1454 - acc: 0.6468 - val_loss: 4.0029 - val_acc: 0.6420\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.8615 - acc: 0.6531 - val_loss: 3.7284 - val_acc: 0.6590\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.5996 - acc: 0.6593 - val_loss: 3.4779 - val_acc: 0.6590\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 3.3602 - acc: 0.6624 - val_loss: 3.2500 - val_acc: 0.6740\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.1425 - acc: 0.6635 - val_loss: 3.0417 - val_acc: 0.6780\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.9463 - acc: 0.6691 - val_loss: 2.8538 - val_acc: 0.6790\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.7710 - acc: 0.6711 - val_loss: 2.6896 - val_acc: 0.6810\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.6167 - acc: 0.6717 - val_loss: 2.5434 - val_acc: 0.6810\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.4825 - acc: 0.6703 - val_loss: 2.4194 - val_acc: 0.6810\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.3687 - acc: 0.6731 - val_loss: 2.3143 - val_acc: 0.6780\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.2740 - acc: 0.6723 - val_loss: 2.2270 - val_acc: 0.6820\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.1980 - acc: 0.6713 - val_loss: 2.1602 - val_acc: 0.6810\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1386 - acc: 0.6725 - val_loss: 2.1087 - val_acc: 0.6800\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0942 - acc: 0.6736 - val_loss: 2.0704 - val_acc: 0.6760\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0616 - acc: 0.6712 - val_loss: 2.0393 - val_acc: 0.6720\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0343 - acc: 0.6735 - val_loss: 2.0143 - val_acc: 0.6710\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0114 - acc: 0.6720 - val_loss: 1.9902 - val_acc: 0.6790\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9903 - acc: 0.6745 - val_loss: 1.9708 - val_acc: 0.6780\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9712 - acc: 0.6747 - val_loss: 1.9519 - val_acc: 0.6830\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9536 - acc: 0.6751 - val_loss: 1.9332 - val_acc: 0.6780\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9369 - acc: 0.6753 - val_loss: 1.9168 - val_acc: 0.6820\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9213 - acc: 0.6769 - val_loss: 1.9012 - val_acc: 0.6770\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9061 - acc: 0.6791 - val_loss: 1.8871 - val_acc: 0.6790\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8920 - acc: 0.6776 - val_loss: 1.8731 - val_acc: 0.6830\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8783 - acc: 0.6800 - val_loss: 1.8585 - val_acc: 0.6820\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8650 - acc: 0.6796 - val_loss: 1.8430 - val_acc: 0.6850\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8519 - acc: 0.6815 - val_loss: 1.8301 - val_acc: 0.6900\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8395 - acc: 0.6824 - val_loss: 1.8186 - val_acc: 0.6910\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8277 - acc: 0.6844 - val_loss: 1.8058 - val_acc: 0.6900\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8157 - acc: 0.6853 - val_loss: 1.7949 - val_acc: 0.6930\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8043 - acc: 0.6844 - val_loss: 1.7821 - val_acc: 0.6950\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7931 - acc: 0.6857 - val_loss: 1.7722 - val_acc: 0.6970\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7828 - acc: 0.6863 - val_loss: 1.7612 - val_acc: 0.6880\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7722 - acc: 0.6876 - val_loss: 1.7494 - val_acc: 0.6940\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7616 - acc: 0.6877 - val_loss: 1.7391 - val_acc: 0.6920\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7517 - acc: 0.6887 - val_loss: 1.7376 - val_acc: 0.6940\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7419 - acc: 0.6892 - val_loss: 1.7217 - val_acc: 0.6940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7319 - acc: 0.6887 - val_loss: 1.7170 - val_acc: 0.6940\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7227 - acc: 0.6892 - val_loss: 1.7054 - val_acc: 0.6970\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7136 - acc: 0.6903 - val_loss: 1.6913 - val_acc: 0.6980\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7041 - acc: 0.6924 - val_loss: 1.6814 - val_acc: 0.6940\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6951 - acc: 0.6928 - val_loss: 1.6735 - val_acc: 0.7040\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6864 - acc: 0.6944 - val_loss: 1.6635 - val_acc: 0.7020\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6778 - acc: 0.6957 - val_loss: 1.6551 - val_acc: 0.7050\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6691 - acc: 0.6960 - val_loss: 1.6475 - val_acc: 0.7070\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6604 - acc: 0.6987 - val_loss: 1.6402 - val_acc: 0.7010\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6518 - acc: 0.6996 - val_loss: 1.6361 - val_acc: 0.7010\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6436 - acc: 0.6987 - val_loss: 1.6220 - val_acc: 0.7000\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6354 - acc: 0.7015 - val_loss: 1.6156 - val_acc: 0.7080\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6274 - acc: 0.7015 - val_loss: 1.6048 - val_acc: 0.7010\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6198 - acc: 0.7019 - val_loss: 1.6047 - val_acc: 0.7080\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6122 - acc: 0.7016 - val_loss: 1.5891 - val_acc: 0.7060\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6044 - acc: 0.7020 - val_loss: 1.5813 - val_acc: 0.7070\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5962 - acc: 0.7048 - val_loss: 1.5755 - val_acc: 0.7060\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5886 - acc: 0.7039 - val_loss: 1.5679 - val_acc: 0.7060\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5816 - acc: 0.7048 - val_loss: 1.5609 - val_acc: 0.7080\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5739 - acc: 0.7051 - val_loss: 1.5524 - val_acc: 0.7100\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5670 - acc: 0.7047 - val_loss: 1.5454 - val_acc: 0.7080\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5596 - acc: 0.7049 - val_loss: 1.5392 - val_acc: 0.7070\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5523 - acc: 0.7080 - val_loss: 1.5329 - val_acc: 0.7170\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5455 - acc: 0.7063 - val_loss: 1.5254 - val_acc: 0.7150\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5384 - acc: 0.7087 - val_loss: 1.5181 - val_acc: 0.7130\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5314 - acc: 0.7088 - val_loss: 1.5132 - val_acc: 0.7110\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5249 - acc: 0.7084 - val_loss: 1.5042 - val_acc: 0.7140\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5176 - acc: 0.7093 - val_loss: 1.5022 - val_acc: 0.7070\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5115 - acc: 0.7108 - val_loss: 1.4923 - val_acc: 0.7130\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5045 - acc: 0.7117 - val_loss: 1.4951 - val_acc: 0.7090\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4983 - acc: 0.7115 - val_loss: 1.4801 - val_acc: 0.7140\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4912 - acc: 0.7127 - val_loss: 1.4728 - val_acc: 0.7140\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4852 - acc: 0.7104 - val_loss: 1.4667 - val_acc: 0.7170\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4784 - acc: 0.7136 - val_loss: 1.4594 - val_acc: 0.7080\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4724 - acc: 0.7137 - val_loss: 1.4551 - val_acc: 0.7140\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4658 - acc: 0.7149 - val_loss: 1.4498 - val_acc: 0.7160\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4602 - acc: 0.7151 - val_loss: 1.4421 - val_acc: 0.7170\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4538 - acc: 0.7163 - val_loss: 1.4346 - val_acc: 0.7170\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4470 - acc: 0.7149 - val_loss: 1.4340 - val_acc: 0.7200\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4414 - acc: 0.7152 - val_loss: 1.4272 - val_acc: 0.7180\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4362 - acc: 0.7188 - val_loss: 1.4224 - val_acc: 0.7150\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4301 - acc: 0.7153 - val_loss: 1.4136 - val_acc: 0.7190\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4246 - acc: 0.7169 - val_loss: 1.4046 - val_acc: 0.7200\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4182 - acc: 0.7196 - val_loss: 1.4068 - val_acc: 0.7190\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4132 - acc: 0.7200 - val_loss: 1.3998 - val_acc: 0.7190\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4072 - acc: 0.7176 - val_loss: 1.3891 - val_acc: 0.7180\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4016 - acc: 0.7204 - val_loss: 1.3859 - val_acc: 0.7220\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3966 - acc: 0.7208 - val_loss: 1.3825 - val_acc: 0.7210\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3906 - acc: 0.7225 - val_loss: 1.3804 - val_acc: 0.7230\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3854 - acc: 0.7219 - val_loss: 1.3692 - val_acc: 0.7220\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3802 - acc: 0.7215 - val_loss: 1.3651 - val_acc: 0.7200\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3750 - acc: 0.7201 - val_loss: 1.3593 - val_acc: 0.7210\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3703 - acc: 0.7241 - val_loss: 1.3563 - val_acc: 0.7220\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3650 - acc: 0.7241 - val_loss: 1.3489 - val_acc: 0.7250\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3596 - acc: 0.7249 - val_loss: 1.3460 - val_acc: 0.7250\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3543 - acc: 0.7255 - val_loss: 1.3375 - val_acc: 0.7250\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3497 - acc: 0.7248 - val_loss: 1.3340 - val_acc: 0.7270\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3450 - acc: 0.7249 - val_loss: 1.3339 - val_acc: 0.7220\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3404 - acc: 0.7247 - val_loss: 1.3267 - val_acc: 0.7210\n",
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3356 - acc: 0.7267 - val_loss: 1.3237 - val_acc: 0.7160\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3309 - acc: 0.7299 - val_loss: 1.3153 - val_acc: 0.7290\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3262 - acc: 0.7273 - val_loss: 1.3085 - val_acc: 0.7270\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3214 - acc: 0.7275 - val_loss: 1.3051 - val_acc: 0.7210\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3167 - acc: 0.7279 - val_loss: 1.3032 - val_acc: 0.7250\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3118 - acc: 0.7288 - val_loss: 1.2985 - val_acc: 0.7240\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3073 - acc: 0.7301 - val_loss: 1.2923 - val_acc: 0.7240\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3030 - acc: 0.7299 - val_loss: 1.2887 - val_acc: 0.7270\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2984 - acc: 0.7301 - val_loss: 1.2880 - val_acc: 0.7230\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2947 - acc: 0.7303 - val_loss: 1.2849 - val_acc: 0.7180\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2899 - acc: 0.7319 - val_loss: 1.2752 - val_acc: 0.7220\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2858 - acc: 0.7301 - val_loss: 1.2707 - val_acc: 0.7260\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2819 - acc: 0.7319 - val_loss: 1.2682 - val_acc: 0.7240\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2771 - acc: 0.7319 - val_loss: 1.2628 - val_acc: 0.7270\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2734 - acc: 0.7331 - val_loss: 1.2590 - val_acc: 0.7290\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2691 - acc: 0.7335 - val_loss: 1.2582 - val_acc: 0.7250\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2655 - acc: 0.7348 - val_loss: 1.2505 - val_acc: 0.7250\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2614 - acc: 0.7351 - val_loss: 1.2478 - val_acc: 0.7320\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2575 - acc: 0.7325 - val_loss: 1.2434 - val_acc: 0.7270\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2538 - acc: 0.7355 - val_loss: 1.2403 - val_acc: 0.7290\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2497 - acc: 0.7355 - val_loss: 1.2362 - val_acc: 0.7290\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2456 - acc: 0.7341 - val_loss: 1.2344 - val_acc: 0.7310\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2418 - acc: 0.7356 - val_loss: 1.2292 - val_acc: 0.7350\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2380 - acc: 0.7364 - val_loss: 1.2267 - val_acc: 0.7320\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2344 - acc: 0.7365 - val_loss: 1.2225 - val_acc: 0.7280\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2306 - acc: 0.7371 - val_loss: 1.2170 - val_acc: 0.7280\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2272 - acc: 0.7349 - val_loss: 1.2162 - val_acc: 0.7320\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2233 - acc: 0.7385 - val_loss: 1.2114 - val_acc: 0.7290\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2199 - acc: 0.7385 - val_loss: 1.2098 - val_acc: 0.7310\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2166 - acc: 0.7379 - val_loss: 1.2034 - val_acc: 0.7340\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2128 - acc: 0.7393 - val_loss: 1.2085 - val_acc: 0.7230\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2096 - acc: 0.7389 - val_loss: 1.1977 - val_acc: 0.7310\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2066 - acc: 0.7391 - val_loss: 1.1958 - val_acc: 0.7320\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2030 - acc: 0.7389 - val_loss: 1.1909 - val_acc: 0.7340\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1992 - acc: 0.7396 - val_loss: 1.1875 - val_acc: 0.7350\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1969 - acc: 0.7380 - val_loss: 1.1904 - val_acc: 0.7290\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1932 - acc: 0.7395 - val_loss: 1.1811 - val_acc: 0.7340\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1896 - acc: 0.7405 - val_loss: 1.1802 - val_acc: 0.7370\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1861 - acc: 0.7408 - val_loss: 1.1754 - val_acc: 0.7300\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1834 - acc: 0.7409 - val_loss: 1.1755 - val_acc: 0.7320\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1803 - acc: 0.7425 - val_loss: 1.1680 - val_acc: 0.7360\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1766 - acc: 0.7424 - val_loss: 1.1751 - val_acc: 0.7280\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1752 - acc: 0.7417 - val_loss: 1.1609 - val_acc: 0.7340\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1716 - acc: 0.7423 - val_loss: 1.1612 - val_acc: 0.7360\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1687 - acc: 0.7409 - val_loss: 1.1594 - val_acc: 0.7370\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1658 - acc: 0.7441 - val_loss: 1.1571 - val_acc: 0.7350\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1629 - acc: 0.7419 - val_loss: 1.1572 - val_acc: 0.7390\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1598 - acc: 0.7429 - val_loss: 1.1513 - val_acc: 0.7430\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1577 - acc: 0.7429 - val_loss: 1.1462 - val_acc: 0.7390\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1546 - acc: 0.7427 - val_loss: 1.1453 - val_acc: 0.7410\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1517 - acc: 0.7431 - val_loss: 1.1406 - val_acc: 0.7410\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1490 - acc: 0.7461 - val_loss: 1.1447 - val_acc: 0.7300\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1470 - acc: 0.7456 - val_loss: 1.1377 - val_acc: 0.7410\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1438 - acc: 0.7455 - val_loss: 1.1356 - val_acc: 0.7440\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1415 - acc: 0.7457 - val_loss: 1.1324 - val_acc: 0.7390\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1394 - acc: 0.7433 - val_loss: 1.1291 - val_acc: 0.7370\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1362 - acc: 0.7455 - val_loss: 1.1279 - val_acc: 0.7440\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1352 - acc: 0.7449 - val_loss: 1.1235 - val_acc: 0.7370\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1320 - acc: 0.7447 - val_loss: 1.1211 - val_acc: 0.7380\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1293 - acc: 0.7463 - val_loss: 1.1236 - val_acc: 0.7380\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1279 - acc: 0.7457 - val_loss: 1.1186 - val_acc: 0.7400\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1250 - acc: 0.7467 - val_loss: 1.1250 - val_acc: 0.7390\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1234 - acc: 0.7436 - val_loss: 1.1210 - val_acc: 0.7360\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1214 - acc: 0.7468 - val_loss: 1.1116 - val_acc: 0.7430\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1188 - acc: 0.7488 - val_loss: 1.1138 - val_acc: 0.7380\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1174 - acc: 0.7473 - val_loss: 1.1087 - val_acc: 0.7440\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1145 - acc: 0.7479 - val_loss: 1.1078 - val_acc: 0.7410\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1129 - acc: 0.7463 - val_loss: 1.1084 - val_acc: 0.7450\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1110 - acc: 0.7476 - val_loss: 1.1017 - val_acc: 0.7440\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1081 - acc: 0.7471 - val_loss: 1.1027 - val_acc: 0.7410\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1065 - acc: 0.7489 - val_loss: 1.0996 - val_acc: 0.7380\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1045 - acc: 0.7473 - val_loss: 1.0977 - val_acc: 0.7430\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1028 - acc: 0.7496 - val_loss: 1.1026 - val_acc: 0.7370\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1010 - acc: 0.7475 - val_loss: 1.0954 - val_acc: 0.7370\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0997 - acc: 0.7484 - val_loss: 1.0904 - val_acc: 0.7450\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0972 - acc: 0.7500 - val_loss: 1.0884 - val_acc: 0.7410\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0953 - acc: 0.7503 - val_loss: 1.0877 - val_acc: 0.7400\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0937 - acc: 0.7480 - val_loss: 1.0867 - val_acc: 0.7390\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0922 - acc: 0.7485 - val_loss: 1.0866 - val_acc: 0.7430\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0896 - acc: 0.7497 - val_loss: 1.0832 - val_acc: 0.7440\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0879 - acc: 0.7495 - val_loss: 1.0827 - val_acc: 0.7440\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0863 - acc: 0.7511 - val_loss: 1.0830 - val_acc: 0.7430\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0848 - acc: 0.7515 - val_loss: 1.0847 - val_acc: 0.7440\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0835 - acc: 0.7508 - val_loss: 1.0759 - val_acc: 0.7420\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0813 - acc: 0.7508 - val_loss: 1.0765 - val_acc: 0.7400\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0800 - acc: 0.7508 - val_loss: 1.0759 - val_acc: 0.7480\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0782 - acc: 0.7513 - val_loss: 1.0711 - val_acc: 0.7430\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0765 - acc: 0.7496 - val_loss: 1.0708 - val_acc: 0.7420\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0753 - acc: 0.7517 - val_loss: 1.0696 - val_acc: 0.7420\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0740 - acc: 0.7512 - val_loss: 1.0685 - val_acc: 0.7400\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0720 - acc: 0.7521 - val_loss: 1.0647 - val_acc: 0.7430\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0696 - acc: 0.7523 - val_loss: 1.0644 - val_acc: 0.7400\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0693 - acc: 0.7537 - val_loss: 1.0667 - val_acc: 0.7420\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0681 - acc: 0.7524 - val_loss: 1.0655 - val_acc: 0.7390\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0656 - acc: 0.7537 - val_loss: 1.0602 - val_acc: 0.7420\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0650 - acc: 0.7543 - val_loss: 1.0602 - val_acc: 0.7420\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0624 - acc: 0.7532 - val_loss: 1.0579 - val_acc: 0.7480\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0619 - acc: 0.7535 - val_loss: 1.0581 - val_acc: 0.7390\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0599 - acc: 0.7533 - val_loss: 1.0526 - val_acc: 0.7450\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0584 - acc: 0.7539 - val_loss: 1.0534 - val_acc: 0.7440\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0572 - acc: 0.7547 - val_loss: 1.0521 - val_acc: 0.7430\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0560 - acc: 0.7552 - val_loss: 1.0489 - val_acc: 0.7490\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0544 - acc: 0.7543 - val_loss: 1.0516 - val_acc: 0.7460\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0530 - acc: 0.7552 - val_loss: 1.0571 - val_acc: 0.7400\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0526 - acc: 0.7544 - val_loss: 1.0492 - val_acc: 0.7450\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0509 - acc: 0.7527 - val_loss: 1.0506 - val_acc: 0.7450\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0493 - acc: 0.7540 - val_loss: 1.0441 - val_acc: 0.7420\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0479 - acc: 0.7555 - val_loss: 1.0413 - val_acc: 0.7430\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0465 - acc: 0.7545 - val_loss: 1.0415 - val_acc: 0.7440\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0449 - acc: 0.7561 - val_loss: 1.0418 - val_acc: 0.7440\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0438 - acc: 0.7551 - val_loss: 1.0408 - val_acc: 0.7460\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0421 - acc: 0.7563 - val_loss: 1.0380 - val_acc: 0.7540\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0410 - acc: 0.7564 - val_loss: 1.0371 - val_acc: 0.7440\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0400 - acc: 0.7569 - val_loss: 1.0419 - val_acc: 0.7370\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0390 - acc: 0.7565 - val_loss: 1.0352 - val_acc: 0.7450\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0372 - acc: 0.7571 - val_loss: 1.0390 - val_acc: 0.7400\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0359 - acc: 0.7575 - val_loss: 1.0348 - val_acc: 0.7490\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0357 - acc: 0.7561 - val_loss: 1.0300 - val_acc: 0.7450\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0334 - acc: 0.7564 - val_loss: 1.0296 - val_acc: 0.7510\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0328 - acc: 0.7564 - val_loss: 1.0358 - val_acc: 0.7510\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0323 - acc: 0.7555 - val_loss: 1.0284 - val_acc: 0.7460\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0306 - acc: 0.7579 - val_loss: 1.0279 - val_acc: 0.7490\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0292 - acc: 0.7588 - val_loss: 1.0257 - val_acc: 0.7470\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0285 - acc: 0.7576 - val_loss: 1.0270 - val_acc: 0.7450\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0269 - acc: 0.7560 - val_loss: 1.0248 - val_acc: 0.7420\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0257 - acc: 0.7575 - val_loss: 1.0254 - val_acc: 0.7450\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0244 - acc: 0.7569 - val_loss: 1.0243 - val_acc: 0.7450\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0233 - acc: 0.7599 - val_loss: 1.0241 - val_acc: 0.7460\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0225 - acc: 0.7581 - val_loss: 1.0215 - val_acc: 0.7440\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0215 - acc: 0.7583 - val_loss: 1.0181 - val_acc: 0.7440\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0202 - acc: 0.7609 - val_loss: 1.0187 - val_acc: 0.7470\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0196 - acc: 0.7559 - val_loss: 1.0175 - val_acc: 0.7430\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0179 - acc: 0.7597 - val_loss: 1.0187 - val_acc: 0.7550\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0173 - acc: 0.7612 - val_loss: 1.0198 - val_acc: 0.7440\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0165 - acc: 0.7596 - val_loss: 1.0164 - val_acc: 0.7520\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0160 - acc: 0.7591 - val_loss: 1.0137 - val_acc: 0.7450\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0143 - acc: 0.7608 - val_loss: 1.0162 - val_acc: 0.7460\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0129 - acc: 0.7603 - val_loss: 1.0109 - val_acc: 0.7510\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0121 - acc: 0.7615 - val_loss: 1.0209 - val_acc: 0.7530\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0118 - acc: 0.7607 - val_loss: 1.0087 - val_acc: 0.7530\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0103 - acc: 0.7604 - val_loss: 1.0091 - val_acc: 0.7490\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0092 - acc: 0.7607 - val_loss: 1.0086 - val_acc: 0.7460\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0081 - acc: 0.7613 - val_loss: 1.0151 - val_acc: 0.7430\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0073 - acc: 0.7589 - val_loss: 1.0057 - val_acc: 0.7450\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0056 - acc: 0.7624 - val_loss: 1.0074 - val_acc: 0.7530\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0054 - acc: 0.7617 - val_loss: 1.0050 - val_acc: 0.7430\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0038 - acc: 0.7629 - val_loss: 1.0060 - val_acc: 0.7520\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0037 - acc: 0.7595 - val_loss: 1.0063 - val_acc: 0.7470\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0023 - acc: 0.7624 - val_loss: 1.0027 - val_acc: 0.7550\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0012 - acc: 0.7624 - val_loss: 1.0099 - val_acc: 0.7470\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0015 - acc: 0.7605 - val_loss: 1.0015 - val_acc: 0.7520\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9996 - acc: 0.7640 - val_loss: 0.9987 - val_acc: 0.7490\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9989 - acc: 0.7629 - val_loss: 0.9976 - val_acc: 0.7500\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9978 - acc: 0.7636 - val_loss: 1.0029 - val_acc: 0.7480\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9971 - acc: 0.7621 - val_loss: 0.9973 - val_acc: 0.7540\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9967 - acc: 0.7612 - val_loss: 0.9938 - val_acc: 0.7510\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9952 - acc: 0.7636 - val_loss: 0.9967 - val_acc: 0.7530\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9945 - acc: 0.7635 - val_loss: 1.0000 - val_acc: 0.7500\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9937 - acc: 0.7629 - val_loss: 0.9950 - val_acc: 0.7470\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9926 - acc: 0.7643 - val_loss: 0.9954 - val_acc: 0.7530\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9927 - acc: 0.7635 - val_loss: 0.9956 - val_acc: 0.7470\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9908 - acc: 0.7643 - val_loss: 0.9973 - val_acc: 0.7450\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9906 - acc: 0.7633 - val_loss: 0.9918 - val_acc: 0.7570\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9895 - acc: 0.7609 - val_loss: 0.9884 - val_acc: 0.7470\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9884 - acc: 0.7655 - val_loss: 0.9925 - val_acc: 0.7500\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9876 - acc: 0.7648 - val_loss: 0.9932 - val_acc: 0.7490\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9868 - acc: 0.7620 - val_loss: 0.9950 - val_acc: 0.7450\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9861 - acc: 0.7667 - val_loss: 0.9908 - val_acc: 0.7520\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9855 - acc: 0.7635 - val_loss: 0.9861 - val_acc: 0.7460\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9847 - acc: 0.7668 - val_loss: 0.9927 - val_acc: 0.7400\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9841 - acc: 0.7645 - val_loss: 0.9864 - val_acc: 0.7560\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9829 - acc: 0.7653 - val_loss: 0.9829 - val_acc: 0.7460\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9823 - acc: 0.7655 - val_loss: 0.9877 - val_acc: 0.7510\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9815 - acc: 0.7645 - val_loss: 0.9833 - val_acc: 0.7570\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9808 - acc: 0.7655 - val_loss: 0.9834 - val_acc: 0.7520\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9789 - acc: 0.7656 - val_loss: 0.9922 - val_acc: 0.7470\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9784 - acc: 0.7665 - val_loss: 0.9815 - val_acc: 0.7480\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9785 - acc: 0.7648 - val_loss: 0.9781 - val_acc: 0.7530\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9774 - acc: 0.7657 - val_loss: 0.9808 - val_acc: 0.7510\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9768 - acc: 0.7657 - val_loss: 0.9815 - val_acc: 0.7480\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9760 - acc: 0.7667 - val_loss: 0.9817 - val_acc: 0.7510\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9747 - acc: 0.7656 - val_loss: 0.9809 - val_acc: 0.7530\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9749 - acc: 0.7655 - val_loss: 0.9872 - val_acc: 0.7500\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9742 - acc: 0.7675 - val_loss: 0.9758 - val_acc: 0.7460\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9733 - acc: 0.7692 - val_loss: 0.9749 - val_acc: 0.7570\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9723 - acc: 0.7675 - val_loss: 0.9831 - val_acc: 0.7460\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9721 - acc: 0.7671 - val_loss: 0.9876 - val_acc: 0.7440\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9712 - acc: 0.7688 - val_loss: 0.9750 - val_acc: 0.7560\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9705 - acc: 0.7691 - val_loss: 0.9740 - val_acc: 0.7520\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9695 - acc: 0.7700 - val_loss: 0.9739 - val_acc: 0.7550\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9689 - acc: 0.7691 - val_loss: 0.9769 - val_acc: 0.7460\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9683 - acc: 0.7680 - val_loss: 0.9741 - val_acc: 0.7440\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9672 - acc: 0.7667 - val_loss: 0.9804 - val_acc: 0.7460\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9665 - acc: 0.7671 - val_loss: 0.9692 - val_acc: 0.7480\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9658 - acc: 0.7677 - val_loss: 0.9884 - val_acc: 0.7550\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9668 - acc: 0.7697 - val_loss: 0.9716 - val_acc: 0.7540\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9652 - acc: 0.7655 - val_loss: 0.9746 - val_acc: 0.7520\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9644 - acc: 0.7688 - val_loss: 0.9681 - val_acc: 0.7560\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9641 - acc: 0.7685 - val_loss: 0.9718 - val_acc: 0.7510\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9636 - acc: 0.7688 - val_loss: 0.9729 - val_acc: 0.7540\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9627 - acc: 0.7673 - val_loss: 0.9664 - val_acc: 0.7490\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9621 - acc: 0.7689 - val_loss: 0.9716 - val_acc: 0.7520\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9616 - acc: 0.7699 - val_loss: 0.9635 - val_acc: 0.7540\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9616 - acc: 0.7688 - val_loss: 0.9703 - val_acc: 0.7440\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9603 - acc: 0.7695 - val_loss: 0.9697 - val_acc: 0.7480\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9605 - acc: 0.7669 - val_loss: 0.9734 - val_acc: 0.7450\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9597 - acc: 0.7693 - val_loss: 0.9655 - val_acc: 0.7500\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9586 - acc: 0.7687 - val_loss: 0.9644 - val_acc: 0.7510\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9577 - acc: 0.7683 - val_loss: 0.9633 - val_acc: 0.7580\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9576 - acc: 0.7681 - val_loss: 0.9751 - val_acc: 0.7540\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9574 - acc: 0.7683 - val_loss: 0.9652 - val_acc: 0.7600\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9572 - acc: 0.7700 - val_loss: 0.9627 - val_acc: 0.7550\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9555 - acc: 0.7677 - val_loss: 0.9716 - val_acc: 0.7520\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9556 - acc: 0.7693 - val_loss: 0.9616 - val_acc: 0.7630\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9555 - acc: 0.7679 - val_loss: 0.9629 - val_acc: 0.7540\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9541 - acc: 0.7692 - val_loss: 0.9640 - val_acc: 0.7460\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9541 - acc: 0.7700 - val_loss: 0.9772 - val_acc: 0.7480\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9538 - acc: 0.7713 - val_loss: 0.9612 - val_acc: 0.7500\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9530 - acc: 0.7688 - val_loss: 0.9576 - val_acc: 0.7620\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9510 - acc: 0.7699 - val_loss: 0.9588 - val_acc: 0.7510\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9512 - acc: 0.7692 - val_loss: 0.9565 - val_acc: 0.7580\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9517 - acc: 0.7689 - val_loss: 0.9629 - val_acc: 0.7490\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9504 - acc: 0.7700 - val_loss: 0.9594 - val_acc: 0.7610\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9503 - acc: 0.7680 - val_loss: 0.9571 - val_acc: 0.7550\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9496 - acc: 0.7681 - val_loss: 0.9551 - val_acc: 0.7600\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9488 - acc: 0.7712 - val_loss: 0.9659 - val_acc: 0.7510\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9487 - acc: 0.7713 - val_loss: 0.9705 - val_acc: 0.7460\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9483 - acc: 0.7709 - val_loss: 0.9521 - val_acc: 0.7580\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9468 - acc: 0.7705 - val_loss: 0.9535 - val_acc: 0.7620\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9469 - acc: 0.7689 - val_loss: 0.9570 - val_acc: 0.7630\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9463 - acc: 0.7717 - val_loss: 0.9548 - val_acc: 0.7530\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9472 - acc: 0.7719 - val_loss: 0.9545 - val_acc: 0.7610\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9461 - acc: 0.7701 - val_loss: 0.9621 - val_acc: 0.7510\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9448 - acc: 0.7716 - val_loss: 0.9535 - val_acc: 0.7650\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9450 - acc: 0.7705 - val_loss: 0.9510 - val_acc: 0.7590\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9444 - acc: 0.7703 - val_loss: 0.9564 - val_acc: 0.7510\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9434 - acc: 0.7708 - val_loss: 0.9491 - val_acc: 0.7660\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9426 - acc: 0.7720 - val_loss: 0.9536 - val_acc: 0.7530\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9417 - acc: 0.7707 - val_loss: 0.9525 - val_acc: 0.7530\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9429 - acc: 0.7719 - val_loss: 0.9516 - val_acc: 0.7600\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9419 - acc: 0.7729 - val_loss: 0.9532 - val_acc: 0.7620\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9450 - acc: 0.770 - 0s 44us/step - loss: 0.9427 - acc: 0.7716 - val_loss: 0.9465 - val_acc: 0.7610\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9407 - acc: 0.7715 - val_loss: 0.9475 - val_acc: 0.7570\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9408 - acc: 0.7723 - val_loss: 0.9450 - val_acc: 0.7630\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9394 - acc: 0.7712 - val_loss: 0.9461 - val_acc: 0.7660\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9392 - acc: 0.7729 - val_loss: 0.9468 - val_acc: 0.7550\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9388 - acc: 0.7712 - val_loss: 0.9449 - val_acc: 0.7540\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9387 - acc: 0.7731 - val_loss: 0.9479 - val_acc: 0.7640\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9381 - acc: 0.7711 - val_loss: 0.9654 - val_acc: 0.7400\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9390 - acc: 0.7708 - val_loss: 0.9449 - val_acc: 0.7600\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9373 - acc: 0.7705 - val_loss: 0.9459 - val_acc: 0.7630\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9374 - acc: 0.7713 - val_loss: 0.9447 - val_acc: 0.7660\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9356 - acc: 0.7717 - val_loss: 0.9426 - val_acc: 0.7650\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9351 - acc: 0.7747 - val_loss: 0.9450 - val_acc: 0.7640\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9352 - acc: 0.7708 - val_loss: 0.9525 - val_acc: 0.7530\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9345 - acc: 0.7724 - val_loss: 0.9514 - val_acc: 0.7500\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9351 - acc: 0.7739 - val_loss: 0.9410 - val_acc: 0.7650\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9336 - acc: 0.7701 - val_loss: 0.9415 - val_acc: 0.7610\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9341 - acc: 0.7712 - val_loss: 0.9454 - val_acc: 0.7580\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9335 - acc: 0.7724 - val_loss: 0.9444 - val_acc: 0.7510\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9326 - acc: 0.7729 - val_loss: 0.9418 - val_acc: 0.7610\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9327 - acc: 0.7732 - val_loss: 0.9391 - val_acc: 0.7640\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9313 - acc: 0.7736 - val_loss: 0.9389 - val_acc: 0.7680\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9310 - acc: 0.7736 - val_loss: 0.9373 - val_acc: 0.7640\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9310 - acc: 0.7708 - val_loss: 0.9374 - val_acc: 0.7620\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9302 - acc: 0.7748 - val_loss: 0.9408 - val_acc: 0.7600\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9297 - acc: 0.7739 - val_loss: 0.9381 - val_acc: 0.7620\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9301 - acc: 0.7727 - val_loss: 0.9401 - val_acc: 0.7540\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9296 - acc: 0.7729 - val_loss: 0.9441 - val_acc: 0.7550\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9287 - acc: 0.7724 - val_loss: 0.9380 - val_acc: 0.7680\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9281 - acc: 0.7731 - val_loss: 0.9398 - val_acc: 0.7630\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9281 - acc: 0.7731 - val_loss: 0.9389 - val_acc: 0.7570\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9277 - acc: 0.7737 - val_loss: 0.9360 - val_acc: 0.7630\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9273 - acc: 0.7733 - val_loss: 0.9399 - val_acc: 0.7600\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9268 - acc: 0.7725 - val_loss: 0.9360 - val_acc: 0.7590\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9270 - acc: 0.7736 - val_loss: 0.9385 - val_acc: 0.7550\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9268 - acc: 0.7729 - val_loss: 0.9420 - val_acc: 0.7660\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9260 - acc: 0.7745 - val_loss: 0.9486 - val_acc: 0.7580\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9260 - acc: 0.7741 - val_loss: 0.9360 - val_acc: 0.7610\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9244 - acc: 0.7737 - val_loss: 0.9329 - val_acc: 0.7590\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9237 - acc: 0.7745 - val_loss: 0.9386 - val_acc: 0.7650\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9235 - acc: 0.7752 - val_loss: 0.9405 - val_acc: 0.7580\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9235 - acc: 0.7735 - val_loss: 0.9440 - val_acc: 0.7600\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9241 - acc: 0.7725 - val_loss: 0.9371 - val_acc: 0.7580\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9232 - acc: 0.7736 - val_loss: 0.9312 - val_acc: 0.7650\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9228 - acc: 0.7744 - val_loss: 0.9327 - val_acc: 0.7630\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9222 - acc: 0.7721 - val_loss: 0.9306 - val_acc: 0.7700\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9211 - acc: 0.7735 - val_loss: 0.9347 - val_acc: 0.7560\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9210 - acc: 0.7735 - val_loss: 0.9288 - val_acc: 0.7670\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9204 - acc: 0.7720 - val_loss: 0.9274 - val_acc: 0.7680\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9204 - acc: 0.7763 - val_loss: 0.9447 - val_acc: 0.7620\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9207 - acc: 0.7756 - val_loss: 0.9429 - val_acc: 0.7510\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9194 - acc: 0.7733 - val_loss: 0.9497 - val_acc: 0.7530\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9207 - acc: 0.7737 - val_loss: 0.9272 - val_acc: 0.7650\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9191 - acc: 0.7760 - val_loss: 0.9390 - val_acc: 0.7590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9193 - acc: 0.7736 - val_loss: 0.9308 - val_acc: 0.7650\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9188 - acc: 0.7751 - val_loss: 0.9294 - val_acc: 0.7620\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9179 - acc: 0.7748 - val_loss: 0.9351 - val_acc: 0.7590\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9184 - acc: 0.7747 - val_loss: 0.9257 - val_acc: 0.7670\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9167 - acc: 0.7748 - val_loss: 0.9266 - val_acc: 0.7690\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9168 - acc: 0.7740 - val_loss: 0.9312 - val_acc: 0.7640\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9165 - acc: 0.7756 - val_loss: 0.9301 - val_acc: 0.7620\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9163 - acc: 0.7763 - val_loss: 0.9313 - val_acc: 0.7630\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9161 - acc: 0.7743 - val_loss: 0.9274 - val_acc: 0.7700\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9154 - acc: 0.7759 - val_loss: 0.9281 - val_acc: 0.7530\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9152 - acc: 0.7751 - val_loss: 0.9285 - val_acc: 0.7660\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9149 - acc: 0.7756 - val_loss: 0.9289 - val_acc: 0.7560\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9145 - acc: 0.7756 - val_loss: 0.9243 - val_acc: 0.7690\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9138 - acc: 0.7768 - val_loss: 0.9295 - val_acc: 0.7560\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9137 - acc: 0.7743 - val_loss: 0.9252 - val_acc: 0.7630\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9131 - acc: 0.7760 - val_loss: 0.9238 - val_acc: 0.7650\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9133 - acc: 0.7751 - val_loss: 0.9230 - val_acc: 0.7660\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9129 - acc: 0.7724 - val_loss: 0.9237 - val_acc: 0.7600\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9137 - acc: 0.7729 - val_loss: 0.9236 - val_acc: 0.7670\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9113 - acc: 0.7757 - val_loss: 0.9300 - val_acc: 0.7560\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9125 - acc: 0.7745 - val_loss: 0.9280 - val_acc: 0.7640\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9113 - acc: 0.7727 - val_loss: 0.9250 - val_acc: 0.7630\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9114 - acc: 0.7753 - val_loss: 0.9233 - val_acc: 0.7580\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9115 - acc: 0.7728 - val_loss: 0.9228 - val_acc: 0.7650\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9119 - acc: 0.7747 - val_loss: 0.9248 - val_acc: 0.7570\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9101 - acc: 0.7753 - val_loss: 0.9370 - val_acc: 0.7540\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9103 - acc: 0.7777 - val_loss: 0.9350 - val_acc: 0.7510\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9098 - acc: 0.7752 - val_loss: 0.9286 - val_acc: 0.7580\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9102 - acc: 0.7740 - val_loss: 0.9217 - val_acc: 0.7650\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9085 - acc: 0.7751 - val_loss: 0.9227 - val_acc: 0.7610\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9097 - acc: 0.7771 - val_loss: 0.9202 - val_acc: 0.7650\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9076 - acc: 0.7768 - val_loss: 0.9242 - val_acc: 0.7530\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9082 - acc: 0.7761 - val_loss: 0.9350 - val_acc: 0.7640\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9085 - acc: 0.7753 - val_loss: 0.9177 - val_acc: 0.7690\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9064 - acc: 0.7777 - val_loss: 0.9183 - val_acc: 0.7660\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9064 - acc: 0.7748 - val_loss: 0.9190 - val_acc: 0.7610\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9062 - acc: 0.7765 - val_loss: 0.9218 - val_acc: 0.7620\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9070 - acc: 0.7749 - val_loss: 0.9217 - val_acc: 0.7680\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9063 - acc: 0.7764 - val_loss: 0.9162 - val_acc: 0.7660\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9058 - acc: 0.7761 - val_loss: 0.9189 - val_acc: 0.7640\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9059 - acc: 0.7767 - val_loss: 0.9195 - val_acc: 0.7640\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9049 - acc: 0.7780 - val_loss: 0.9222 - val_acc: 0.7580\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9043 - acc: 0.7783 - val_loss: 0.9232 - val_acc: 0.7610\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9049 - acc: 0.7772 - val_loss: 0.9200 - val_acc: 0.7640\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9046 - acc: 0.7784 - val_loss: 0.9249 - val_acc: 0.7580\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9037 - acc: 0.7787 - val_loss: 0.9218 - val_acc: 0.7600\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9035 - acc: 0.7779 - val_loss: 0.9174 - val_acc: 0.7690\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9032 - acc: 0.7773 - val_loss: 0.9202 - val_acc: 0.7660\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9029 - acc: 0.7767 - val_loss: 0.9192 - val_acc: 0.7660\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9029 - acc: 0.7755 - val_loss: 0.9171 - val_acc: 0.7670\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9024 - acc: 0.7784 - val_loss: 0.9144 - val_acc: 0.7670\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9019 - acc: 0.7775 - val_loss: 0.9148 - val_acc: 0.7670\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9014 - acc: 0.7773 - val_loss: 0.9198 - val_acc: 0.7520\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9014 - acc: 0.7776 - val_loss: 0.9144 - val_acc: 0.7610\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9016 - acc: 0.7767 - val_loss: 0.9141 - val_acc: 0.7670\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9006 - acc: 0.7781 - val_loss: 0.9152 - val_acc: 0.7620\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9012 - acc: 0.7745 - val_loss: 0.9206 - val_acc: 0.7540\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8994 - acc: 0.7773 - val_loss: 0.9142 - val_acc: 0.7640\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9009 - acc: 0.7777 - val_loss: 0.9187 - val_acc: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9009 - acc: 0.7785 - val_loss: 0.9140 - val_acc: 0.7620\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8999 - acc: 0.7775 - val_loss: 0.9120 - val_acc: 0.7670\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8987 - acc: 0.7792 - val_loss: 0.9345 - val_acc: 0.7490\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9005 - acc: 0.7776 - val_loss: 0.9133 - val_acc: 0.7700\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8992 - acc: 0.7785 - val_loss: 0.9191 - val_acc: 0.7580\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8987 - acc: 0.7788 - val_loss: 0.9105 - val_acc: 0.7640\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8982 - acc: 0.7780 - val_loss: 0.9209 - val_acc: 0.7580\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8986 - acc: 0.7792 - val_loss: 0.9102 - val_acc: 0.7650\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8975 - acc: 0.7781 - val_loss: 0.9128 - val_acc: 0.7690\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8969 - acc: 0.7769 - val_loss: 0.9133 - val_acc: 0.7560\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8982 - acc: 0.7776 - val_loss: 0.9196 - val_acc: 0.7680\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8989 - acc: 0.7796 - val_loss: 0.9117 - val_acc: 0.7690\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8973 - acc: 0.7791 - val_loss: 0.9126 - val_acc: 0.7600\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8967 - acc: 0.7781 - val_loss: 0.9115 - val_acc: 0.7590\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8961 - acc: 0.7791 - val_loss: 0.9100 - val_acc: 0.7660\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8964 - acc: 0.7771 - val_loss: 0.9090 - val_acc: 0.7660\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8956 - acc: 0.7788 - val_loss: 0.9136 - val_acc: 0.7560\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8959 - acc: 0.7793 - val_loss: 0.9222 - val_acc: 0.7490\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8955 - acc: 0.7792 - val_loss: 0.9107 - val_acc: 0.7670\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8950 - acc: 0.7809 - val_loss: 0.9091 - val_acc: 0.7700\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8940 - acc: 0.7764 - val_loss: 0.9135 - val_acc: 0.7680\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8941 - acc: 0.7785 - val_loss: 0.9099 - val_acc: 0.7600\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8928 - acc: 0.7781 - val_loss: 0.9082 - val_acc: 0.7660\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7785 - val_loss: 0.9113 - val_acc: 0.7620\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8926 - acc: 0.7807 - val_loss: 0.9074 - val_acc: 0.7710\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8939 - acc: 0.7772 - val_loss: 0.9051 - val_acc: 0.7680\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8925 - acc: 0.7781 - val_loss: 0.9156 - val_acc: 0.7650\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7793 - val_loss: 0.9136 - val_acc: 0.7660\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7797 - val_loss: 0.9077 - val_acc: 0.7670\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8929 - acc: 0.7780 - val_loss: 0.9101 - val_acc: 0.7660\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8921 - acc: 0.7788 - val_loss: 0.9118 - val_acc: 0.7580\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8917 - acc: 0.7781 - val_loss: 0.9090 - val_acc: 0.7700\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8913 - acc: 0.7797 - val_loss: 0.9057 - val_acc: 0.7640\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8922 - acc: 0.7783 - val_loss: 0.9152 - val_acc: 0.7520\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8911 - acc: 0.7776 - val_loss: 0.9081 - val_acc: 0.7620\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8917 - acc: 0.7783 - val_loss: 0.9075 - val_acc: 0.7630\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8913 - acc: 0.7773 - val_loss: 0.9061 - val_acc: 0.7660\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8903 - acc: 0.7791 - val_loss: 0.9047 - val_acc: 0.7680\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8894 - acc: 0.7792 - val_loss: 0.9070 - val_acc: 0.7620\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8905 - acc: 0.7783 - val_loss: 0.9043 - val_acc: 0.7690\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8899 - acc: 0.7793 - val_loss: 0.9092 - val_acc: 0.7670\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8912 - acc: 0.7788 - val_loss: 0.9194 - val_acc: 0.7610\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8890 - acc: 0.7799 - val_loss: 0.9028 - val_acc: 0.7710\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8892 - acc: 0.7804 - val_loss: 0.9060 - val_acc: 0.7610\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8895 - acc: 0.7793 - val_loss: 0.9112 - val_acc: 0.7640\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8895 - acc: 0.7792 - val_loss: 0.9020 - val_acc: 0.7700\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8881 - acc: 0.7779 - val_loss: 0.9022 - val_acc: 0.7650\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8873 - acc: 0.7775 - val_loss: 0.9023 - val_acc: 0.7700\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8875 - acc: 0.7791 - val_loss: 0.9098 - val_acc: 0.7650\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8873 - acc: 0.7819 - val_loss: 0.8998 - val_acc: 0.7700\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8862 - acc: 0.7803 - val_loss: 0.9126 - val_acc: 0.7620\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8866 - acc: 0.7807 - val_loss: 0.9049 - val_acc: 0.7710\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8878 - acc: 0.7803 - val_loss: 0.9050 - val_acc: 0.7620\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8868 - acc: 0.7817 - val_loss: 0.9017 - val_acc: 0.7600\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8861 - acc: 0.7813 - val_loss: 0.9029 - val_acc: 0.7710\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8866 - acc: 0.7809 - val_loss: 0.9049 - val_acc: 0.7630\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8865 - acc: 0.7808 - val_loss: 0.9038 - val_acc: 0.7690\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8863 - acc: 0.7809 - val_loss: 0.9004 - val_acc: 0.7650\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8860 - acc: 0.7809 - val_loss: 0.9061 - val_acc: 0.7650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8848 - acc: 0.7819 - val_loss: 0.9010 - val_acc: 0.7680\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8845 - acc: 0.7815 - val_loss: 0.9010 - val_acc: 0.7630\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8846 - acc: 0.7801 - val_loss: 0.9220 - val_acc: 0.7600\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8864 - acc: 0.7803 - val_loss: 0.9060 - val_acc: 0.7660\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8851 - acc: 0.7827 - val_loss: 0.9006 - val_acc: 0.7710\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8846 - acc: 0.7805 - val_loss: 0.9013 - val_acc: 0.7700\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8848 - acc: 0.7805 - val_loss: 0.9001 - val_acc: 0.7680\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8849 - acc: 0.7813 - val_loss: 0.8981 - val_acc: 0.7710\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8842 - acc: 0.7795 - val_loss: 0.9019 - val_acc: 0.7670\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8833 - acc: 0.7808 - val_loss: 0.8983 - val_acc: 0.7640\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8842 - acc: 0.7808 - val_loss: 0.8989 - val_acc: 0.7600\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8830 - acc: 0.7813 - val_loss: 0.9020 - val_acc: 0.7700\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8840 - acc: 0.7820 - val_loss: 0.9038 - val_acc: 0.7600\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8820 - acc: 0.7812 - val_loss: 0.9092 - val_acc: 0.7610\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8823 - acc: 0.7809 - val_loss: 0.8995 - val_acc: 0.7680\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8825 - acc: 0.7797 - val_loss: 0.9331 - val_acc: 0.7580\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8835 - acc: 0.7816 - val_loss: 0.8995 - val_acc: 0.7680\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8814 - acc: 0.7823 - val_loss: 0.9037 - val_acc: 0.7570\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8822 - acc: 0.7815 - val_loss: 0.8956 - val_acc: 0.7760\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8825 - acc: 0.7803 - val_loss: 0.9003 - val_acc: 0.7600\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8818 - acc: 0.7804 - val_loss: 0.8978 - val_acc: 0.7700\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8806 - acc: 0.7816 - val_loss: 0.8948 - val_acc: 0.7750\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8805 - acc: 0.7829 - val_loss: 0.9006 - val_acc: 0.7690\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8809 - acc: 0.7815 - val_loss: 0.8999 - val_acc: 0.7710\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8807 - acc: 0.7827 - val_loss: 0.8971 - val_acc: 0.7670\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8810 - acc: 0.7813 - val_loss: 0.8964 - val_acc: 0.7720\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8806 - acc: 0.7815 - val_loss: 0.9139 - val_acc: 0.7570\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8803 - acc: 0.7831 - val_loss: 0.8958 - val_acc: 0.7720\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8803 - acc: 0.7825 - val_loss: 0.9025 - val_acc: 0.7760\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8799 - acc: 0.7827 - val_loss: 0.8957 - val_acc: 0.7680\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8796 - acc: 0.7824 - val_loss: 0.8983 - val_acc: 0.7640\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8794 - acc: 0.7823 - val_loss: 0.8961 - val_acc: 0.7670\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8789 - acc: 0.781 - 0s 36us/step - loss: 0.8798 - acc: 0.7816 - val_loss: 0.8959 - val_acc: 0.7700\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8789 - acc: 0.7817 - val_loss: 0.9079 - val_acc: 0.7670\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8780 - acc: 0.7828 - val_loss: 0.8931 - val_acc: 0.7660\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8794 - acc: 0.7817 - val_loss: 0.9093 - val_acc: 0.7570\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8796 - acc: 0.7829 - val_loss: 0.9164 - val_acc: 0.7620\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8798 - acc: 0.7803 - val_loss: 0.8987 - val_acc: 0.7660\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8778 - acc: 0.7827 - val_loss: 0.9133 - val_acc: 0.7520\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8782 - acc: 0.7827 - val_loss: 0.8960 - val_acc: 0.7670\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8773 - acc: 0.7812 - val_loss: 0.9114 - val_acc: 0.7610\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8788 - acc: 0.7813 - val_loss: 0.8938 - val_acc: 0.7700\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8767 - acc: 0.7835 - val_loss: 0.9009 - val_acc: 0.7680\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8770 - acc: 0.7852 - val_loss: 0.8958 - val_acc: 0.7730\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8760 - acc: 0.7825 - val_loss: 0.8958 - val_acc: 0.7720\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8765 - acc: 0.7804 - val_loss: 0.8937 - val_acc: 0.7740\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8771 - acc: 0.7832 - val_loss: 0.8957 - val_acc: 0.7680\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8775 - acc: 0.7815 - val_loss: 0.8947 - val_acc: 0.7720\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8756 - acc: 0.7839 - val_loss: 0.8932 - val_acc: 0.7710\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8765 - acc: 0.7823 - val_loss: 0.8949 - val_acc: 0.7690\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8767 - acc: 0.7823 - val_loss: 0.8966 - val_acc: 0.7650\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8755 - acc: 0.7831 - val_loss: 0.8939 - val_acc: 0.7660\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8768 - acc: 0.7817 - val_loss: 0.8959 - val_acc: 0.7700\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8773 - acc: 0.7820 - val_loss: 0.8954 - val_acc: 0.7690\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8759 - acc: 0.7800 - val_loss: 0.9110 - val_acc: 0.7630\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8749 - acc: 0.7825 - val_loss: 0.8942 - val_acc: 0.7660\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8749 - acc: 0.7833 - val_loss: 0.8952 - val_acc: 0.7670\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8751 - acc: 0.7833 - val_loss: 0.8930 - val_acc: 0.7680\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8733 - acc: 0.7859 - val_loss: 0.8997 - val_acc: 0.7570\n",
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8744 - acc: 0.7843 - val_loss: 0.8956 - val_acc: 0.7630\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8740 - acc: 0.7827 - val_loss: 0.8945 - val_acc: 0.7690\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8744 - acc: 0.7829 - val_loss: 0.8928 - val_acc: 0.7660\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8747 - acc: 0.7817 - val_loss: 0.8969 - val_acc: 0.7700\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8738 - acc: 0.7829 - val_loss: 0.8980 - val_acc: 0.7670\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8726 - acc: 0.7845 - val_loss: 0.9185 - val_acc: 0.7620\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8742 - acc: 0.7841 - val_loss: 0.8928 - val_acc: 0.7720\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8733 - acc: 0.7853 - val_loss: 0.8896 - val_acc: 0.7700\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8725 - acc: 0.7813 - val_loss: 0.8941 - val_acc: 0.7650\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8729 - acc: 0.7840 - val_loss: 0.9022 - val_acc: 0.7620\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8732 - acc: 0.7815 - val_loss: 0.8955 - val_acc: 0.7600\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8727 - acc: 0.7836 - val_loss: 0.8885 - val_acc: 0.7710\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8718 - acc: 0.7837 - val_loss: 0.8904 - val_acc: 0.7700\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8715 - acc: 0.7837 - val_loss: 0.8936 - val_acc: 0.7660\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8722 - acc: 0.7815 - val_loss: 0.8932 - val_acc: 0.7660\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8735 - acc: 0.7813 - val_loss: 0.8924 - val_acc: 0.7670\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8708 - acc: 0.7855 - val_loss: 0.8929 - val_acc: 0.7710\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8717 - acc: 0.7835 - val_loss: 0.8895 - val_acc: 0.7740\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8707 - acc: 0.7851 - val_loss: 0.9019 - val_acc: 0.7520\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8706 - acc: 0.7836 - val_loss: 0.8904 - val_acc: 0.7680\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8717 - acc: 0.7833 - val_loss: 0.8919 - val_acc: 0.7690\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8708 - acc: 0.7847 - val_loss: 0.8945 - val_acc: 0.7730\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8701 - acc: 0.7823 - val_loss: 0.8916 - val_acc: 0.7700\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8695 - acc: 0.7828 - val_loss: 0.8978 - val_acc: 0.7550\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8715 - acc: 0.7839 - val_loss: 0.9058 - val_acc: 0.7530\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8698 - acc: 0.7824 - val_loss: 0.8964 - val_acc: 0.7680\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8692 - acc: 0.7851 - val_loss: 0.8976 - val_acc: 0.7690\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8707 - acc: 0.7844 - val_loss: 0.8921 - val_acc: 0.7620\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8695 - acc: 0.7840 - val_loss: 0.8903 - val_acc: 0.7720\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8695 - acc: 0.7839 - val_loss: 0.8968 - val_acc: 0.7590\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8697 - acc: 0.7833 - val_loss: 0.8901 - val_acc: 0.7650\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8685 - acc: 0.7851 - val_loss: 0.8876 - val_acc: 0.7730\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8696 - acc: 0.7837 - val_loss: 0.9169 - val_acc: 0.7610\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8693 - acc: 0.7831 - val_loss: 0.8896 - val_acc: 0.7770\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8682 - acc: 0.7828 - val_loss: 0.8882 - val_acc: 0.7670\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8690 - acc: 0.7803 - val_loss: 0.8892 - val_acc: 0.7680\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8687 - acc: 0.7824 - val_loss: 0.8875 - val_acc: 0.7770\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8687 - acc: 0.7839 - val_loss: 0.8971 - val_acc: 0.7640\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8680 - acc: 0.7848 - val_loss: 0.8876 - val_acc: 0.7740\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8674 - acc: 0.7847 - val_loss: 0.9123 - val_acc: 0.7540\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8684 - acc: 0.7851 - val_loss: 0.8878 - val_acc: 0.7690\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8683 - acc: 0.7845 - val_loss: 0.8944 - val_acc: 0.7520\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8670 - acc: 0.7845 - val_loss: 0.8923 - val_acc: 0.7690\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8683 - acc: 0.7837 - val_loss: 0.8988 - val_acc: 0.7670\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8667 - acc: 0.7852 - val_loss: 0.8988 - val_acc: 0.7700\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8675 - acc: 0.7851 - val_loss: 0.8952 - val_acc: 0.7700\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8669 - acc: 0.7843 - val_loss: 0.8854 - val_acc: 0.7690\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8669 - acc: 0.7835 - val_loss: 0.8869 - val_acc: 0.7720\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8668 - acc: 0.7824 - val_loss: 0.9022 - val_acc: 0.7690\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8677 - acc: 0.7831 - val_loss: 0.8940 - val_acc: 0.7730\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8654 - acc: 0.7859 - val_loss: 0.8909 - val_acc: 0.7730\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8671 - acc: 0.7841 - val_loss: 0.8867 - val_acc: 0.7720\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8649 - acc: 0.7823 - val_loss: 0.8942 - val_acc: 0.7660\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8665 - acc: 0.7848 - val_loss: 0.8929 - val_acc: 0.7740\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8648 - acc: 0.7843 - val_loss: 0.8940 - val_acc: 0.7730\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8660 - acc: 0.7836 - val_loss: 0.8881 - val_acc: 0.7660\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8643 - acc: 0.7864 - val_loss: 0.8970 - val_acc: 0.7670\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8653 - acc: 0.7841 - val_loss: 0.8851 - val_acc: 0.7760\n",
      "Epoch 649/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8649 - acc: 0.7836 - val_loss: 0.8896 - val_acc: 0.7760\n",
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8640 - acc: 0.7841 - val_loss: 0.9073 - val_acc: 0.7550\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8668 - acc: 0.7856 - val_loss: 0.8923 - val_acc: 0.7630\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8658 - acc: 0.7857 - val_loss: 0.8906 - val_acc: 0.7760\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8642 - acc: 0.7855 - val_loss: 0.8862 - val_acc: 0.7560\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8643 - acc: 0.7841 - val_loss: 0.8937 - val_acc: 0.7660\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8647 - acc: 0.7861 - val_loss: 0.9076 - val_acc: 0.7620\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8655 - acc: 0.7853 - val_loss: 0.8893 - val_acc: 0.7700\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8645 - acc: 0.7832 - val_loss: 0.8870 - val_acc: 0.7620\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8646 - acc: 0.7843 - val_loss: 0.8907 - val_acc: 0.7620\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8649 - acc: 0.7856 - val_loss: 0.8835 - val_acc: 0.7760\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8632 - acc: 0.7845 - val_loss: 0.8857 - val_acc: 0.7690\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8629 - acc: 0.7832 - val_loss: 0.8880 - val_acc: 0.7660\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8625 - acc: 0.7867 - val_loss: 0.8946 - val_acc: 0.7670\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8632 - acc: 0.7867 - val_loss: 0.9199 - val_acc: 0.7470\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8648 - acc: 0.7844 - val_loss: 0.8890 - val_acc: 0.7710\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8626 - acc: 0.7867 - val_loss: 0.8937 - val_acc: 0.7620\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8642 - acc: 0.7847 - val_loss: 0.8906 - val_acc: 0.7690\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8622 - acc: 0.7860 - val_loss: 0.8846 - val_acc: 0.7710\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8621 - acc: 0.7855 - val_loss: 0.8870 - val_acc: 0.7670\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8630 - acc: 0.7855 - val_loss: 0.8827 - val_acc: 0.7710\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8619 - acc: 0.7847 - val_loss: 0.8839 - val_acc: 0.7690\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8638 - acc: 0.7860 - val_loss: 0.8872 - val_acc: 0.7560\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8615 - acc: 0.7848 - val_loss: 0.8840 - val_acc: 0.7770\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8606 - acc: 0.7865 - val_loss: 0.8842 - val_acc: 0.7670\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8600 - acc: 0.7875 - val_loss: 0.8842 - val_acc: 0.7740\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8627 - acc: 0.7840 - val_loss: 0.8854 - val_acc: 0.7720\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8617 - acc: 0.7847 - val_loss: 0.8840 - val_acc: 0.7750\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8607 - acc: 0.7855 - val_loss: 0.8852 - val_acc: 0.7750\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8610 - acc: 0.7844 - val_loss: 0.8845 - val_acc: 0.7740\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8604 - acc: 0.7869 - val_loss: 0.8938 - val_acc: 0.7700\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8603 - acc: 0.7865 - val_loss: 0.8844 - val_acc: 0.7720\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8603 - acc: 0.7857 - val_loss: 0.8933 - val_acc: 0.7670\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8608 - acc: 0.7860 - val_loss: 0.8809 - val_acc: 0.7740\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8609 - acc: 0.7847 - val_loss: 0.8915 - val_acc: 0.7640\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8605 - acc: 0.7871 - val_loss: 0.8904 - val_acc: 0.7600\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8605 - acc: 0.7867 - val_loss: 0.8816 - val_acc: 0.7760\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8610 - acc: 0.7833 - val_loss: 0.8809 - val_acc: 0.7740\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8585 - acc: 0.7853 - val_loss: 0.8882 - val_acc: 0.7700\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8602 - acc: 0.7852 - val_loss: 0.8810 - val_acc: 0.7760\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8584 - acc: 0.7859 - val_loss: 0.8847 - val_acc: 0.7660\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8604 - acc: 0.7852 - val_loss: 0.8929 - val_acc: 0.7670\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8613 - acc: 0.7852 - val_loss: 0.8826 - val_acc: 0.7730\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8590 - acc: 0.7857 - val_loss: 0.8836 - val_acc: 0.7750\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8594 - acc: 0.7865 - val_loss: 0.8780 - val_acc: 0.7750\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8584 - acc: 0.7837 - val_loss: 0.8789 - val_acc: 0.7720\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8569 - acc: 0.7856 - val_loss: 0.9019 - val_acc: 0.7580\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8586 - acc: 0.7864 - val_loss: 0.8833 - val_acc: 0.7680\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8587 - acc: 0.7867 - val_loss: 0.8867 - val_acc: 0.7670\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8589 - acc: 0.7884 - val_loss: 0.8854 - val_acc: 0.7630\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8580 - acc: 0.7863 - val_loss: 0.8930 - val_acc: 0.7600\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8586 - acc: 0.7864 - val_loss: 0.8877 - val_acc: 0.7720\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8589 - acc: 0.7864 - val_loss: 0.8848 - val_acc: 0.7670\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8573 - acc: 0.7863 - val_loss: 0.8811 - val_acc: 0.7800\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8580 - acc: 0.7875 - val_loss: 0.8932 - val_acc: 0.7660\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8581 - acc: 0.7884 - val_loss: 0.8864 - val_acc: 0.7630\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8568 - acc: 0.7860 - val_loss: 0.8818 - val_acc: 0.7700\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8582 - acc: 0.7865 - val_loss: 0.8820 - val_acc: 0.7780\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8578 - acc: 0.7875 - val_loss: 0.8899 - val_acc: 0.7640\n",
      "Epoch 708/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8567 - acc: 0.7865 - val_loss: 0.8828 - val_acc: 0.7630\n",
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8559 - acc: 0.7872 - val_loss: 0.8801 - val_acc: 0.7730\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8576 - acc: 0.7848 - val_loss: 0.9000 - val_acc: 0.7660\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8566 - acc: 0.7859 - val_loss: 0.8860 - val_acc: 0.7610\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8575 - acc: 0.7876 - val_loss: 0.8777 - val_acc: 0.7790\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8551 - acc: 0.7885 - val_loss: 0.8820 - val_acc: 0.7680\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8577 - acc: 0.7881 - val_loss: 0.8791 - val_acc: 0.7750\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8567 - acc: 0.7861 - val_loss: 0.8865 - val_acc: 0.7650\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8570 - acc: 0.7872 - val_loss: 0.8798 - val_acc: 0.7790\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8555 - acc: 0.7875 - val_loss: 0.8794 - val_acc: 0.7690\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8556 - acc: 0.7888 - val_loss: 0.8802 - val_acc: 0.7760\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8554 - acc: 0.7879 - val_loss: 0.8878 - val_acc: 0.7720\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8542 - acc: 0.7892 - val_loss: 0.8827 - val_acc: 0.7630\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8553 - acc: 0.7869 - val_loss: 0.8850 - val_acc: 0.7740\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8555 - acc: 0.7856 - val_loss: 0.8881 - val_acc: 0.7750\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8531 - acc: 0.7888 - val_loss: 0.8868 - val_acc: 0.7720\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8547 - acc: 0.7872 - val_loss: 0.8804 - val_acc: 0.7740\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8552 - acc: 0.7867 - val_loss: 0.8799 - val_acc: 0.7660\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8550 - acc: 0.7875 - val_loss: 0.8773 - val_acc: 0.7790\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8541 - acc: 0.7867 - val_loss: 0.8778 - val_acc: 0.7790\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8556 - acc: 0.7880 - val_loss: 0.8768 - val_acc: 0.7740\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8545 - acc: 0.7876 - val_loss: 0.8845 - val_acc: 0.7570\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8534 - acc: 0.7879 - val_loss: 0.8850 - val_acc: 0.7700\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8546 - acc: 0.7881 - val_loss: 0.8927 - val_acc: 0.7640\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8547 - acc: 0.7888 - val_loss: 0.8765 - val_acc: 0.7770\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8542 - acc: 0.7877 - val_loss: 0.8846 - val_acc: 0.7620\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8529 - acc: 0.7884 - val_loss: 0.8797 - val_acc: 0.7690\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8535 - acc: 0.7871 - val_loss: 0.8825 - val_acc: 0.7690\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8541 - acc: 0.7883 - val_loss: 0.9048 - val_acc: 0.7520\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8541 - acc: 0.7857 - val_loss: 0.8794 - val_acc: 0.7720\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8543 - acc: 0.7877 - val_loss: 0.8813 - val_acc: 0.7730\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8521 - acc: 0.7877 - val_loss: 0.8790 - val_acc: 0.7610\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.7861 - val_loss: 0.8771 - val_acc: 0.7790\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8522 - acc: 0.7885 - val_loss: 0.8771 - val_acc: 0.7750\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.7875 - val_loss: 0.8800 - val_acc: 0.7770\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8522 - acc: 0.7863 - val_loss: 0.8798 - val_acc: 0.7620\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8517 - acc: 0.7883 - val_loss: 0.8860 - val_acc: 0.7620\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8534 - acc: 0.7867 - val_loss: 0.8824 - val_acc: 0.7680\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8517 - acc: 0.7897 - val_loss: 0.8847 - val_acc: 0.7670\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8513 - acc: 0.7877 - val_loss: 0.8908 - val_acc: 0.7630\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8533 - acc: 0.7873 - val_loss: 0.8952 - val_acc: 0.7640\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8515 - acc: 0.7877 - val_loss: 0.9081 - val_acc: 0.7580\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8524 - acc: 0.7869 - val_loss: 0.8971 - val_acc: 0.7570\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8524 - acc: 0.7879 - val_loss: 0.8761 - val_acc: 0.7690\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8515 - acc: 0.7881 - val_loss: 0.8807 - val_acc: 0.7690\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8515 - acc: 0.7876 - val_loss: 0.8847 - val_acc: 0.7680\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8525 - acc: 0.7901 - val_loss: 0.8848 - val_acc: 0.7630\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8516 - acc: 0.7891 - val_loss: 0.8805 - val_acc: 0.7660\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8515 - acc: 0.7881 - val_loss: 0.8771 - val_acc: 0.7740\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8502 - acc: 0.7880 - val_loss: 0.8824 - val_acc: 0.7660\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8504 - acc: 0.7897 - val_loss: 0.8898 - val_acc: 0.7520\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8512 - acc: 0.7885 - val_loss: 0.8823 - val_acc: 0.7660\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8509 - acc: 0.7871 - val_loss: 0.8852 - val_acc: 0.7650\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8501 - acc: 0.7884 - val_loss: 0.8866 - val_acc: 0.7630\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8504 - acc: 0.7880 - val_loss: 0.9056 - val_acc: 0.7660\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8526 - acc: 0.7872 - val_loss: 0.8780 - val_acc: 0.7710\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8492 - acc: 0.7908 - val_loss: 0.9256 - val_acc: 0.7600\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8505 - acc: 0.7871 - val_loss: 0.8952 - val_acc: 0.7630\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8510 - acc: 0.7888 - val_loss: 0.8824 - val_acc: 0.7680\n",
      "Epoch 767/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8502 - acc: 0.7888 - val_loss: 0.8754 - val_acc: 0.7660\n",
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8494 - acc: 0.7873 - val_loss: 0.8806 - val_acc: 0.7650\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8499 - acc: 0.7888 - val_loss: 0.8784 - val_acc: 0.7610\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8493 - acc: 0.7896 - val_loss: 0.8749 - val_acc: 0.7730\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8489 - acc: 0.7883 - val_loss: 0.8865 - val_acc: 0.7570\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8503 - acc: 0.7861 - val_loss: 0.8771 - val_acc: 0.7730\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8514 - acc: 0.7860 - val_loss: 0.8727 - val_acc: 0.7780\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8473 - acc: 0.7901 - val_loss: 0.8729 - val_acc: 0.7810\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8500 - acc: 0.7881 - val_loss: 0.8779 - val_acc: 0.7670\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8501 - acc: 0.7884 - val_loss: 0.8944 - val_acc: 0.7730\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8512 - acc: 0.7875 - val_loss: 0.8777 - val_acc: 0.7730\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8491 - acc: 0.7883 - val_loss: 0.8754 - val_acc: 0.7670\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8485 - acc: 0.7908 - val_loss: 0.8877 - val_acc: 0.7600\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8483 - acc: 0.7869 - val_loss: 0.8789 - val_acc: 0.7720\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8484 - acc: 0.7879 - val_loss: 0.8857 - val_acc: 0.7590\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8477 - acc: 0.7885 - val_loss: 0.8737 - val_acc: 0.7710\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8476 - acc: 0.7881 - val_loss: 0.8722 - val_acc: 0.7660\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8476 - acc: 0.7900 - val_loss: 0.8985 - val_acc: 0.7710\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8482 - acc: 0.7865 - val_loss: 0.8861 - val_acc: 0.7640\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8490 - acc: 0.7897 - val_loss: 0.8863 - val_acc: 0.7690\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8481 - acc: 0.7872 - val_loss: 0.8863 - val_acc: 0.7790\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8472 - acc: 0.7884 - val_loss: 0.8747 - val_acc: 0.7760\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8478 - acc: 0.7921 - val_loss: 0.8806 - val_acc: 0.7730\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8463 - acc: 0.7892 - val_loss: 0.9155 - val_acc: 0.7620\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8494 - acc: 0.7888 - val_loss: 0.8986 - val_acc: 0.7580\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8471 - acc: 0.7880 - val_loss: 0.8724 - val_acc: 0.7720\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8464 - acc: 0.7911 - val_loss: 0.8766 - val_acc: 0.7700\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8464 - acc: 0.7901 - val_loss: 0.8803 - val_acc: 0.7680\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8463 - acc: 0.7919 - val_loss: 0.8756 - val_acc: 0.7760\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8457 - acc: 0.7896 - val_loss: 0.8704 - val_acc: 0.7780\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8467 - acc: 0.7897 - val_loss: 0.8741 - val_acc: 0.7680\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8451 - acc: 0.7889 - val_loss: 0.8812 - val_acc: 0.7730\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8475 - acc: 0.7888 - val_loss: 0.8841 - val_acc: 0.7680\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8459 - acc: 0.7901 - val_loss: 0.8758 - val_acc: 0.7640\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8454 - acc: 0.7911 - val_loss: 0.8855 - val_acc: 0.7680\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8478 - acc: 0.7895 - val_loss: 0.8714 - val_acc: 0.7760\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8447 - acc: 0.7931 - val_loss: 0.8870 - val_acc: 0.7630\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8463 - acc: 0.7917 - val_loss: 0.8808 - val_acc: 0.7760\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8469 - acc: 0.7899 - val_loss: 0.8759 - val_acc: 0.7670\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8457 - acc: 0.7880 - val_loss: 0.8780 - val_acc: 0.7730\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8454 - acc: 0.7921 - val_loss: 0.8760 - val_acc: 0.7710\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8456 - acc: 0.7892 - val_loss: 0.8776 - val_acc: 0.7700\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8449 - acc: 0.7893 - val_loss: 0.8746 - val_acc: 0.7680\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8450 - acc: 0.7915 - val_loss: 0.8837 - val_acc: 0.7720\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8455 - acc: 0.7911 - val_loss: 0.8805 - val_acc: 0.7600\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8452 - acc: 0.7899 - val_loss: 0.8708 - val_acc: 0.7720\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8440 - acc: 0.7892 - val_loss: 0.8731 - val_acc: 0.7820\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8433 - acc: 0.7921 - val_loss: 0.8840 - val_acc: 0.7670\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8447 - acc: 0.7893 - val_loss: 0.8840 - val_acc: 0.7670\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8438 - acc: 0.7905 - val_loss: 0.8796 - val_acc: 0.7650\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8442 - acc: 0.7887 - val_loss: 0.8811 - val_acc: 0.7710\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8439 - acc: 0.7887 - val_loss: 0.8814 - val_acc: 0.7660\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8444 - acc: 0.7895 - val_loss: 0.9006 - val_acc: 0.7600\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8439 - acc: 0.7895 - val_loss: 0.8831 - val_acc: 0.7550\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8427 - acc: 0.7885 - val_loss: 0.8728 - val_acc: 0.7700\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8431 - acc: 0.7927 - val_loss: 0.8718 - val_acc: 0.7760\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8430 - acc: 0.7899 - val_loss: 0.8770 - val_acc: 0.7700\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8447 - acc: 0.7880 - val_loss: 0.8745 - val_acc: 0.7650\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8428 - acc: 0.7901 - val_loss: 0.8729 - val_acc: 0.7760\n",
      "Epoch 826/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8425 - acc: 0.7915 - val_loss: 0.8753 - val_acc: 0.7740\n",
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8437 - acc: 0.7893 - val_loss: 0.8756 - val_acc: 0.7630\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8419 - acc: 0.7913 - val_loss: 0.8763 - val_acc: 0.7740\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8435 - acc: 0.7900 - val_loss: 0.8850 - val_acc: 0.7780\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8432 - acc: 0.7911 - val_loss: 0.8700 - val_acc: 0.7760\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8421 - acc: 0.7883 - val_loss: 0.8707 - val_acc: 0.7790\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8446 - acc: 0.7892 - val_loss: 0.8699 - val_acc: 0.7750\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8420 - acc: 0.7919 - val_loss: 0.8727 - val_acc: 0.7700\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8420 - acc: 0.7923 - val_loss: 0.8736 - val_acc: 0.7640\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8415 - acc: 0.7896 - val_loss: 0.8838 - val_acc: 0.7550\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8415 - acc: 0.7917 - val_loss: 0.8744 - val_acc: 0.7670\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8430 - acc: 0.7892 - val_loss: 0.8892 - val_acc: 0.7550\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8431 - acc: 0.7903 - val_loss: 0.8729 - val_acc: 0.7710\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8410 - acc: 0.7931 - val_loss: 0.8717 - val_acc: 0.7700\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8413 - acc: 0.7903 - val_loss: 0.8742 - val_acc: 0.7740\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8417 - acc: 0.7908 - val_loss: 0.8823 - val_acc: 0.7750\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8423 - acc: 0.7908 - val_loss: 0.8700 - val_acc: 0.7700\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8403 - acc: 0.7907 - val_loss: 0.8698 - val_acc: 0.7720\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8405 - acc: 0.7913 - val_loss: 0.8691 - val_acc: 0.7740\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8429 - acc: 0.7896 - val_loss: 0.8698 - val_acc: 0.7740\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8391 - acc: 0.7912 - val_loss: 0.8722 - val_acc: 0.7700\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8410 - acc: 0.7901 - val_loss: 0.9618 - val_acc: 0.7380\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8452 - acc: 0.7873 - val_loss: 0.8821 - val_acc: 0.7790\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8439 - acc: 0.7900 - val_loss: 0.8696 - val_acc: 0.7820\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8402 - acc: 0.7912 - val_loss: 0.8697 - val_acc: 0.7610\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8392 - acc: 0.7917 - val_loss: 0.8730 - val_acc: 0.7640\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8392 - acc: 0.7928 - val_loss: 0.8805 - val_acc: 0.7570\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8410 - acc: 0.7911 - val_loss: 0.8692 - val_acc: 0.7690\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8393 - acc: 0.7912 - val_loss: 0.8701 - val_acc: 0.7720\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8404 - acc: 0.7904 - val_loss: 0.8670 - val_acc: 0.7790\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8417 - acc: 0.7921 - val_loss: 0.8851 - val_acc: 0.7690\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8400 - acc: 0.7905 - val_loss: 0.8999 - val_acc: 0.7610\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8409 - acc: 0.7907 - val_loss: 0.8827 - val_acc: 0.7700\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8418 - acc: 0.7880 - val_loss: 0.8833 - val_acc: 0.7710\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8380 - acc: 0.7939 - val_loss: 0.8869 - val_acc: 0.7600\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8395 - acc: 0.7927 - val_loss: 0.8714 - val_acc: 0.7600\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8386 - acc: 0.7903 - val_loss: 0.8788 - val_acc: 0.7680\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8409 - acc: 0.7909 - val_loss: 0.8762 - val_acc: 0.7700\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8391 - acc: 0.7908 - val_loss: 0.8687 - val_acc: 0.7720\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8390 - acc: 0.7929 - val_loss: 0.8948 - val_acc: 0.7550\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8384 - acc: 0.7925 - val_loss: 0.8860 - val_acc: 0.7680\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8376 - acc: 0.7923 - val_loss: 0.8703 - val_acc: 0.7740\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8389 - acc: 0.7901 - val_loss: 0.8712 - val_acc: 0.7650\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8403 - acc: 0.7921 - val_loss: 0.8751 - val_acc: 0.7640\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8391 - acc: 0.7933 - val_loss: 0.8676 - val_acc: 0.7740\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8390 - acc: 0.7920 - val_loss: 0.8739 - val_acc: 0.7710\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8379 - acc: 0.7924 - val_loss: 0.8740 - val_acc: 0.7750\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8370 - acc: 0.7909 - val_loss: 0.8694 - val_acc: 0.7780\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8384 - acc: 0.7924 - val_loss: 0.8706 - val_acc: 0.7780\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8360 - acc: 0.7919 - val_loss: 0.8774 - val_acc: 0.7750\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8364 - acc: 0.7953 - val_loss: 0.8750 - val_acc: 0.7700\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8387 - acc: 0.7908 - val_loss: 0.8785 - val_acc: 0.7660\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8378 - acc: 0.7920 - val_loss: 0.8692 - val_acc: 0.7620\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8357 - acc: 0.7912 - val_loss: 0.8671 - val_acc: 0.7770\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8378 - acc: 0.7921 - val_loss: 0.8758 - val_acc: 0.7720\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8380 - acc: 0.7924 - val_loss: 0.8648 - val_acc: 0.7760\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8372 - acc: 0.7929 - val_loss: 0.8806 - val_acc: 0.7570\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8369 - acc: 0.7915 - val_loss: 0.8671 - val_acc: 0.7760\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8381 - acc: 0.7924 - val_loss: 0.8780 - val_acc: 0.7590\n",
      "Epoch 885/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8364 - acc: 0.7929 - val_loss: 0.8660 - val_acc: 0.7780\n",
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8360 - acc: 0.7935 - val_loss: 0.8719 - val_acc: 0.7640\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8375 - acc: 0.7937 - val_loss: 0.8741 - val_acc: 0.7720\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8369 - acc: 0.7924 - val_loss: 0.8723 - val_acc: 0.7700\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8362 - acc: 0.7937 - val_loss: 0.8802 - val_acc: 0.7640\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8361 - acc: 0.7883 - val_loss: 0.8674 - val_acc: 0.7750\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8349 - acc: 0.7940 - val_loss: 0.8714 - val_acc: 0.7690\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8356 - acc: 0.7939 - val_loss: 0.8732 - val_acc: 0.7630\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8354 - acc: 0.7935 - val_loss: 0.8716 - val_acc: 0.7700\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8361 - acc: 0.7916 - val_loss: 0.8665 - val_acc: 0.7740\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8353 - acc: 0.7949 - val_loss: 0.8713 - val_acc: 0.7790\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8348 - acc: 0.7917 - val_loss: 0.8746 - val_acc: 0.7700\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8351 - acc: 0.7912 - val_loss: 0.8771 - val_acc: 0.7750\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8370 - acc: 0.7927 - val_loss: 0.8718 - val_acc: 0.7730\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8361 - acc: 0.7931 - val_loss: 0.8860 - val_acc: 0.7580\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8353 - acc: 0.7915 - val_loss: 0.8719 - val_acc: 0.7750\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8356 - acc: 0.7935 - val_loss: 0.8811 - val_acc: 0.7600\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8350 - acc: 0.7932 - val_loss: 0.8670 - val_acc: 0.7700\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8361 - acc: 0.7917 - val_loss: 0.8775 - val_acc: 0.7670\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8366 - acc: 0.7927 - val_loss: 0.8785 - val_acc: 0.7630\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8349 - acc: 0.7931 - val_loss: 0.8660 - val_acc: 0.7820\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8348 - acc: 0.7925 - val_loss: 0.8749 - val_acc: 0.7630\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8355 - acc: 0.7924 - val_loss: 0.8714 - val_acc: 0.7740\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8357 - acc: 0.7931 - val_loss: 0.8642 - val_acc: 0.7800\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8336 - acc: 0.7924 - val_loss: 0.8703 - val_acc: 0.7630\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8331 - acc: 0.7953 - val_loss: 0.8805 - val_acc: 0.7660\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8329 - acc: 0.7939 - val_loss: 0.8785 - val_acc: 0.7590\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8348 - acc: 0.7917 - val_loss: 0.8721 - val_acc: 0.7800\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8345 - acc: 0.7949 - val_loss: 0.9010 - val_acc: 0.7670\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8361 - acc: 0.7913 - val_loss: 0.8663 - val_acc: 0.7790\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8332 - acc: 0.7947 - val_loss: 0.8676 - val_acc: 0.7700\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8328 - acc: 0.7931 - val_loss: 0.8873 - val_acc: 0.7640\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8343 - acc: 0.7940 - val_loss: 0.8788 - val_acc: 0.7690\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8333 - acc: 0.7933 - val_loss: 0.9082 - val_acc: 0.7560\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8334 - acc: 0.7937 - val_loss: 0.8792 - val_acc: 0.7650\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8334 - acc: 0.7905 - val_loss: 0.8708 - val_acc: 0.7700\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8318 - acc: 0.7947 - val_loss: 0.8702 - val_acc: 0.7780\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8331 - acc: 0.7961 - val_loss: 0.8727 - val_acc: 0.7710\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8334 - acc: 0.7940 - val_loss: 0.8657 - val_acc: 0.7670\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8320 - acc: 0.7917 - val_loss: 0.8748 - val_acc: 0.7620\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8348 - acc: 0.7956 - val_loss: 0.8673 - val_acc: 0.7710\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8332 - acc: 0.7928 - val_loss: 0.8731 - val_acc: 0.7700\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8332 - acc: 0.7948 - val_loss: 0.8649 - val_acc: 0.7750\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8340 - acc: 0.7935 - val_loss: 0.8692 - val_acc: 0.7730\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8319 - acc: 0.7949 - val_loss: 0.8743 - val_acc: 0.7750\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8353 - acc: 0.7935 - val_loss: 0.8815 - val_acc: 0.7590\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8325 - acc: 0.7924 - val_loss: 0.8768 - val_acc: 0.7660\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8309 - acc: 0.7928 - val_loss: 0.8755 - val_acc: 0.7800\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8327 - acc: 0.7940 - val_loss: 0.8750 - val_acc: 0.7760\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8318 - acc: 0.7943 - val_loss: 0.8758 - val_acc: 0.7630\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8316 - acc: 0.7937 - val_loss: 0.8810 - val_acc: 0.7650\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8310 - acc: 0.7932 - val_loss: 0.8650 - val_acc: 0.7790\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8313 - acc: 0.7931 - val_loss: 0.8739 - val_acc: 0.7790\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8301 - acc: 0.7935 - val_loss: 0.8809 - val_acc: 0.7660\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8326 - acc: 0.7952 - val_loss: 0.8729 - val_acc: 0.7660\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8318 - acc: 0.7915 - val_loss: 0.8750 - val_acc: 0.7770\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8314 - acc: 0.7937 - val_loss: 0.8660 - val_acc: 0.7780\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8322 - acc: 0.7944 - val_loss: 0.8653 - val_acc: 0.7800\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8303 - acc: 0.7969 - val_loss: 0.8620 - val_acc: 0.7780\n",
      "Epoch 944/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8307 - acc: 0.7940 - val_loss: 0.9117 - val_acc: 0.7530\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8319 - acc: 0.7949 - val_loss: 0.8722 - val_acc: 0.7640\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8318 - acc: 0.7956 - val_loss: 0.9060 - val_acc: 0.7530\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8312 - acc: 0.7940 - val_loss: 0.8882 - val_acc: 0.7560\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8293 - acc: 0.7953 - val_loss: 0.8652 - val_acc: 0.7710\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8320 - acc: 0.7960 - val_loss: 0.8838 - val_acc: 0.7720\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8297 - acc: 0.7943 - val_loss: 0.8648 - val_acc: 0.7820\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8288 - acc: 0.7957 - val_loss: 0.8646 - val_acc: 0.7750\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8293 - acc: 0.7955 - val_loss: 0.8815 - val_acc: 0.7700\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8303 - acc: 0.7967 - val_loss: 0.8624 - val_acc: 0.7730\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8296 - acc: 0.7955 - val_loss: 0.8666 - val_acc: 0.7700\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8304 - acc: 0.7936 - val_loss: 0.8820 - val_acc: 0.7740\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8294 - acc: 0.7951 - val_loss: 0.8776 - val_acc: 0.7760\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8311 - acc: 0.7921 - val_loss: 0.9091 - val_acc: 0.7580\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8301 - acc: 0.7952 - val_loss: 0.8609 - val_acc: 0.7770\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8293 - acc: 0.7951 - val_loss: 0.8673 - val_acc: 0.7700\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8315 - acc: 0.7940 - val_loss: 0.8695 - val_acc: 0.7740\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8308 - acc: 0.7948 - val_loss: 0.8662 - val_acc: 0.7700\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8289 - acc: 0.7959 - val_loss: 0.8629 - val_acc: 0.7800\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8269 - acc: 0.7956 - val_loss: 0.8621 - val_acc: 0.7790\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8309 - acc: 0.7948 - val_loss: 0.8687 - val_acc: 0.7730\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8297 - acc: 0.7956 - val_loss: 0.8656 - val_acc: 0.7730\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8275 - acc: 0.7956 - val_loss: 0.8643 - val_acc: 0.7700\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8281 - acc: 0.7937 - val_loss: 0.8645 - val_acc: 0.7780\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8269 - acc: 0.7956 - val_loss: 0.8661 - val_acc: 0.7810\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8286 - acc: 0.7937 - val_loss: 0.8648 - val_acc: 0.7770\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8291 - acc: 0.7952 - val_loss: 0.8656 - val_acc: 0.7810\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8297 - acc: 0.7952 - val_loss: 0.8651 - val_acc: 0.7770\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8281 - acc: 0.7944 - val_loss: 0.8798 - val_acc: 0.7690\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8282 - acc: 0.7987 - val_loss: 0.8627 - val_acc: 0.7750\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8275 - acc: 0.7961 - val_loss: 0.8654 - val_acc: 0.7710\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8281 - acc: 0.7956 - val_loss: 0.8720 - val_acc: 0.7670\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8328 - acc: 0.7943 - val_loss: 0.9056 - val_acc: 0.7610\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8312 - acc: 0.7955 - val_loss: 0.8746 - val_acc: 0.7730\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8295 - acc: 0.7947 - val_loss: 0.8683 - val_acc: 0.7670\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8262 - acc: 0.7971 - val_loss: 0.8672 - val_acc: 0.7750\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8269 - acc: 0.7943 - val_loss: 0.8624 - val_acc: 0.7740\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8274 - acc: 0.7972 - val_loss: 0.8839 - val_acc: 0.7690\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8274 - acc: 0.7949 - val_loss: 0.8687 - val_acc: 0.7670\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8260 - acc: 0.7971 - val_loss: 0.9091 - val_acc: 0.7640\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8273 - acc: 0.7960 - val_loss: 0.8981 - val_acc: 0.7720\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8323 - acc: 0.7923 - val_loss: 0.8765 - val_acc: 0.7760\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8276 - acc: 0.7931 - val_loss: 0.8668 - val_acc: 0.7680\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8286 - acc: 0.7952 - val_loss: 0.8701 - val_acc: 0.7690\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8278 - acc: 0.7963 - val_loss: 0.8693 - val_acc: 0.7620\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8265 - acc: 0.7953 - val_loss: 0.8649 - val_acc: 0.7690\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8269 - acc: 0.7981 - val_loss: 0.8839 - val_acc: 0.7730\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8277 - acc: 0.7940 - val_loss: 0.8716 - val_acc: 0.7770\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8268 - acc: 0.7971 - val_loss: 0.8685 - val_acc: 0.7830\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8255 - acc: 0.7952 - val_loss: 0.8832 - val_acc: 0.7540\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8265 - acc: 0.7960 - val_loss: 0.8651 - val_acc: 0.7650\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8261 - acc: 0.7964 - val_loss: 0.8932 - val_acc: 0.7690\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8298 - acc: 0.7944 - val_loss: 0.8595 - val_acc: 0.7830\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8243 - acc: 0.7968 - val_loss: 0.8693 - val_acc: 0.7730\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8247 - acc: 0.7968 - val_loss: 0.8596 - val_acc: 0.7790\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8254 - acc: 0.7988 - val_loss: 0.8680 - val_acc: 0.7660\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8262 - acc: 0.7965 - val_loss: 0.8655 - val_acc: 0.7780\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4FeXZ+PHvnQWyEAiSsAZIFJQ1bBEEgitSQIp7hepPK6KVqrW1i9r6unV5bS2KW31dsbZUXGhZFLCCUEHZAhKRILJDCEsIkASSkO3+/TGT4yGcbJDDyUnuz3WdK2fmPGfmnjOTuWeeZ+YZUVWMMcYYgJBAB2CMMabhsKRgjDHGw5KCMcYYD0sKxhhjPCwpGGOM8bCkYIwxxsOSQgMhIqEickxEutRn2YZORP4hIo+77y8VkY21KXsa82k0v5k5+85k2ws2lhROk7uDqXiVi0ih1/DNdZ2eqpapagtV3V2fZU+HiFwoIutEJF9EvhGRkf6YT2WqulRVe9fHtERkuYj8yGvafv3NmoLKv6nX+J4iMldEskXksIgsEJHuAQjR1ANLCqfJ3cG0UNUWwG7g+17jZlQuLyJhZz/K0/ZXYC7QEhgL7A1sOKYqIhIiIoH+P24FzAYuANoB64F/n80AGur/VwNZP3USVMEGExH5vYi8KyLviEg+cIuIDBWRlSJyVET2icjzIhLulg8TERWRRHf4H+7nC9wj9hUiklTXsu7nY0TkWxHJFZEXRORzX0d8XkqBXerYrqqbaljWLSIy2mu4mXvEmOz+U3wgIvvd5V4qIj2rmM5IEdnpNTxIRNa7y/QO0NzrszYiMt89Oj0iIvNEpJP72Z+AocD/uWdu03z8ZrHu75YtIjtF5GEREfezySLyXxF51o15u4iMqmb5H3HL5IvIRhEZX+nzH7tnXPki8rWI9HPHdxWR2W4Mh0TkOXf870XkLa/vdxMR9RpeLiK/E5EVwHGgixvzJnce20RkcqUYrnN/yzwR2Soio0RkooisqlTuQRH5oKpl9UVVV6rqm6p6WFVLgGeB3iLSysdvlSoie713lCJyo4isc99fJM5Zap6IHBCRp33Ns2JbEZHfiMh+4DV3/HgRSXfX23IR6eP1nRSv7WmmiLwv31VdThaRpV5lT9peKs27ym3P/fyU9VOX3zPQLCn417XAP3GOpN7F2dneD8QBw4HRwI+r+f4Pgf8BzsE5G/ldXcuKSFvgPeBX7nx3AINriHs1MLVi51UL7wATvYbHAFmq+pU7/CHQHWgPfA38vaYJikhzYA7wJs4yzQGu8SoSgrMj6AJ0BUqA5wBU9UFgBXC3e+b2Mx+z+CsQBZwLXA7cAdzq9fkwYAPQBmcn90Y14X6Lsz5bAX8A/iki7dzlmAg8AtyMc+Z1HXBYnCPbj4CtQCLQGWc91db/Aya508wEDgBXucN3Ai+ISLIbwzCc3/EXQCxwGbAL9+heTq7quYVarJ8aXAxkqmquj88+x1lXl3iN+yHO/wnAC8DTqtoS6AZUl6ASgBY428BPRORCnG1iMs56exOY4x6kNMdZ3tdxtqdZnLw91UWV256XyusneKiqvc7wBewERlYa93vg0xq+90vgffd9GKBAojv8D+D/vMqOB74+jbKTgGVenwmwD/hRFTHdAqThVBtlAsnu+DHAqiq+0wPIBSLc4XeB31RRNs6NPdor9sfd9yOBne77y4E9gHh9d3VFWR/TTQGyvYaXey+j928GhOMk6PO9Pr8HWOS+nwx84/VZS/e7cbXcHr4GrnLfLwbu8VFmBLAfCPXx2e+Bt7yGuzn/qict26M1xPBhxXxxEtrTVZR7DXjCfd8fOASEV1H2pN+0ijJdgCzgxmrKPAW86r6PBQqABHf4C+BRoE0N8xkJFAHNKi3LY5XKbcNJ2JcDuyt9ttJr25sMLPW1vVTeTmu57VW7fhryy84U/GuP94CI9BCRj9yqlDzgSZydZFX2e70vwDkqqmvZjt5xqLPVVnfkcj/wvKrOx9lR/sc94hwGLPL1BVX9Buef7yoRaQGMwz3yE+eqnz+71St5OEfGUP1yV8Sd6cZbYVfFGxGJFpHXRWS3O91PazHNCm2BUO/pue87eQ1X/j2hit9fRH7kVWVxFCdJVsTSGee3qawzTgIsq2XMlVXetsaJyCpxqu2OAqNqEQPA33DOYsA5IHhXnSqgOnPPSv8DPKeq71dT9J/A9eJUnV6Pc7BRsU3eDvQCNovIahEZW810DqhqsddwV+DBivXg/g4dcNZrR07d7vdwGmq57Z3WtBsCSwr+VbkL2ldwjiK7qXN6/CjOkbs/7cM5zQZARISTd36VheEcRaOqc4AHcZLBLcC0ar5XUYV0LbBeVXe642/FOeu4HKd6pVtFKHWJ2+VdN/trIAkY7P6Wl1cqW133vweBMpydiPe069ygLiLnAi8DU3CObmOBb/hu+fYA5/n46h6gq4iE+vjsOE7VVoX2Psp4tzFE4lSz/C/Qzo3hP7WIAVVd7k5jOM76O62qIxFpg7OdfKCqf6qurDrVivuA73Fy1RGqullVJ+Ak7qnALBGJqGpSlYb34Jz1xHq9olT1PXxvT5293tfmN69Q07bnK7agYUnh7IrBqWY5Lk5ja3XtCfXlQ2CgiHzfrce+H4ivpvz7wOMi0tdtDPwGKAYigar+OcFJCmOAu/D6J8dZ5hNADs4/3R9qGfdyIERE7nUb/W4EBlaabgFwxN0hPVrp+wdw2gtO4R4JfwD8UURaiNMo/3OcKoK6aoGzA8jGybmTcc4UKrwO/FpEBoiju4h0xmnzyHFjiBKRSHfHDM7VO5eISGcRiQUeqiGG5kAzN4YyERkHXOH1+RvAZBG5TJyG/wQRucDr87/jJLbjqrqyhnmFi0iE1yvcbVD+D0516SM1fL/COzi/+VC82g1E5P+JSJyqluP8ryhQXstpvgrcI84l1eKu2++LSDTO9hQqIlPc7el6YJDXd9OBZHe7jwQeq2Y+NW17Qc2Swtn1C+A2IB/nrOFdf89QVQ8ANwHP4OyEzgO+xNlR+/In4G2cS1IP45wdTMb5J/5IRFpWMZ9MnLaIizi5wXQ6Th1zFrARp864NnGfwDnruBM4gtNAO9uryDM4Zx457jQXVJrENGCiW43wjI9Z/AQn2e0A/otTjfJ2bWKrFOdXwPM47R37cBLCKq/P38H5Td8F8oB/Aa1VtRSnmq0nzhHubuAG92sLcS7p3OBOd24NMRzF2cH+G2ed3YBzMFDx+Rc4v+PzODvaJZx8lPw20IfanSW8ChR6vV5z5zcQJ/F437/TsZrp/BPnCPsTVT3iNX4ssEmcK/b+AtxUqYqoSqq6CueM7WWcbeZbnDNc7+3pbvezHwDzcf8PVDUD+COwFNgMfFbNrGra9oKanFxlaxo7t7oiC7hBVZcFOh4TeO6R9EGgj6ruCHQ8Z4uIrAWmqeqZXm3VqNiZQhMgIqNFpJV7Wd7/4LQZrA5wWKbhuAf4vLEnBHG6UWnnVh/dgXNW959Ax9XQNMi7AE29SwVm4NQ7bwSucU+nTRMnIpk419lfHehYzoKeONV40ThXY13vVq8aL1Z9ZIwxxsOqj4wxxngEXfVRXFycJiYmBjoMY4wJKmvXrj2kqtVdjg4EYVJITEwkLS0t0GEYY0xQEZFdNZey6iNjjDFeLCkYY4zxsKRgjDHGw5KCMcYYD0sKxhhjPCwpGGOM8bCkYIwxxiPo7lMwxpiGpLCkkC2Ht7Andw/7j+2na2xX+rXrR3z0yfeJqSrHS46TW5RL3ok8jpccp7CkkBNlJwiVUMJDw2ke2pzI8EiahzbnaNFRcgpzOFx4mJyCHHIKcxh3/jhSOqb4dXksKRhjGp2jRUf5fPfnbMzeyIguIxiSMIQQCUFVyczLJKcwh7wTeQB0iulEp5adKCgp4MCxA2w7so2VmStZtXcVoRJKUmwSXWO7EhsRS8vmLRGEwtJCso9n8+nOT/ls12cUlRadEkNsRCzR4dFEhEVwvOQ4OQU5lJSf1pNOPdq3aG9JwRjT+JSVlxEiIThPh60dVWXToU0s27WMjOwMisuKKSkvoXVEaxJaJhAVHsXafWtZkbmCDQc2oF5PxExomUBibCIbDmwg90RujfMKlVD6te/Hun3raBPZhpzCHJ/lesT14MeDfsywzsPo0qoLbaPbsuPIDtIPpLPjyA4KSwspKCkgOjyauKg4nvr8KV4Z9woxzWJo0awFkeGRXPn3K1ly2xJKyko4UXaCwpJCikqLaBXRirioOM6JPIc2kW1oHdmasBD/77KDrpfUlJQUtW4ujGnYdufuZs3eNWw/sp2dR3dyouwE4SHhnCg7wYaDG9hwYAPRzaIZ0WUEAzsMZF/+PrYc3kJWfhZ5J/LIL86nXE9+CmdpeanniDymWQwRYRGEhYRxpOiIZ3yr5q0YkjCE4Z2Hc3HXi+kR14NPtn3CB5s+YO7muUxJmUJyu2SmfDSFRf9vESP/PpLpV09nb95eosKjaN+iPZ1bdWZgh4FEhX/3yGZ5Qtj3i33kFuXS46Ue7Lx/JzHNYzgn8hzP5/qYnvTe1zhvNY2r7XRqS0TWqmqNpxmWFIwxHocLD/PZrs+IDIukZfOWHCs+xp68PRw4doByLUdR/mfJ/zCs8zC+zfmWjjEd6R3fm44xHSksKSSvOI8Ve1aw7cg2zzRbR7QmMjySkrISwkLC6BXfi37t+vHMymfodk43th7eSmxELBe0uYBVe1cxqf8kYprH8Nyq53jgogd4ZuUzPHDRA4gIU1dMZct9Wziv9XmeswxVJacwh9yiXJJaJxEiJ18/U9sd6enscM9kJ322WVIwponxtYMqKi1i48GNrN+/nv3H9hMeGk6IhJCVn8WOozsoLitmRJcRDOs8jI++/Yi/pv2VY8XHqp1Py+Yt6deuHxe0uYCsY1lsPLiRXbm7aBPZhqjwKPbk7WHa96aR2iWVbud0o1VEq2qnV1BSQGRYZJ2qkk7nKPx0pt2YWFIwJohUriooKi2ieWjzKneUR4uOkpGdQUZ2BnfOu5NHL37UacwszCGnIIf9x/Z7jvC969YrRIZFktQ6iYzsjJPGT+gzgZlfz+TzSZ+TW5RLdLNoElom0KFFB099dlhImCeuuh6FN9YdbjCwpGBMgJWUlbAicwWLti/id5/9jl8P+zVR4VFEhkcSFR6FqlJYWkhhSSHFZcWUlpeSmZ9JWlYa3+Z8C0DPuJ5sOrSJwZ0GU1BS4FymWJBDYWnhKfOLCo/yNEq2a9GOzi0707llZ/q07UO/9v3o0qoLpeWllJaX0uqpVp6d84FjB/hizxf0aduH7m26nzRN25k3Hg0iKYjIaOA5IBR4XVWfqvT5s8Bl7mAU0FZVY6ubpiUFEyjyhJD7UC7bj2xnxZ4VrMla4zkaP15ynJhmMbRs3pLismJyT+TyzaFvAOdKlpjmMZ5r0n0JlVDCQsKIj44npWMK/dr1I/9EPjuO7iCnMIfIsEgiwyOZ/c1sfjH0F0xdMZV5E+fRK74XHVp0ICIs4qSzipoaLE3TE/CkICKhwLfAlUAmsAaYqKoZVZS/DxigqpOqm64lBVOfikqL2HV0FzuP7qSotIjw0HDyT+SzaPsi/rP9P+zO3U1YSBjlWn7K1TBto9ty8PhBrulxDbO/mc34C8aTdyKPpTuXcmOvG2kX3Y7Lky7n8qTLPfXqZeVlFJUWUVBSgIgQGRZJRFgEoSGhgVh804TUNin486LXwcBWVd3uBjQTuBrwmRSAicBjfozHNHLHi48za9Ms3k5/mw0HN1BQUkBJWQnd23Tnwo4XMn39dL5//vfZdGgTRwqPUFBS4LMaBpzG1JHnjuTmvjcjCCJCRFgEkWGRdIzpyNDOQ+naqmudGkcBQkNCiW4WTXSz6PpYZGPqnT+TQidgj9dwJjDEV0ER6QokAZ9W8fldwF0AXbp0qd8oTYNQU9VG3ok8/rXpX2TmZVJSVkJpeSkl5SWUlJUwbdU0BrQfwDeHvqGwtJBzW5/L1RdcTXR4NKEhoUxdMZUDxw4QHhLOjqM7GNhhIHGRcUSFR9GyeUsSYxNJap1EVHgUJWUlhIeG07dtX8JDw8/iL2BMw+DPpODrEKqq//oJwAeqWubrQ1V9FXgVnOqj+gnPNCTe9d+Fvy3kvzv/y+gZo5n2vWmsP7Ce9za+R0FJgad8RV8x4SHhJMUm0a5FOy7pegnTVk1j631bCXkyxDPNv4z6C6pKuZZbNY0xNfBnUsgEOnsNJwBZVZSdANzjx1hMParvBsuy8jK+zfmWtKw0bkm+hbZPtyW/OB+An338M2KaxXBz35u5Y8AdDOo4iFAJJeTJEEp/W3rKtJ4d/SzAqdetixAq3yWE6pbBGmRNU+bPhuYwnIbmK4C9OA3NP1TVjZXKXQB8DCRpLYKxhuaGpaod6LHiY2w8uJF9x/Zx4NgBOsZ05IpzryAqPIr0/em8tOYlXlv3Gp1bdia7INvTTUHriNZc2+Nabuh1A73ie9GyeUvO+fM5tpM25gwFvKFZVUtF5F6cHX4o8KaqbhSRJ4E0VZ3rFp0IzKxNQjANy/Hi46y7ax3vfv0uO4/uJKcwh0MFh1i/fz1fHfiKskq1gc1Dm9O9TXe+Pvg1kWGRjO0+lvioeOKi4ujbti8pHVPoEdfjlCoe74RgR/HG+JfdvGbqTFV548s3+PnHPz+pS4Tmoc1pE9WGrPwsHhnxCL9f9nvS7kyjbXRbNuds5qNvP2LtvrWMv2A8kwZM8nQmZozxv4Dfp+AvlhTOLnlCOPrgURbvWMz1713PhxM/5LV1rzFn8xwuT7qcn6T8hPPbnE9S6ySiw6NrvETTjvSNCYyAVx+Z4LIndw+7c3fTu21vWv+pNVvu28KKPSu4vuf1tPtLO8+duOPeGUez0GY8M+oZ7r/o/lN6pKyJJQRjGjZLCk2cPCFs/MlGhrw+xFMVFB0eTfcXnD5w2ka35ceDfswPev+AVhGtKCgpoGNMRxJaJgQybGOMn1hSaKIqqnGOPHiEC1+7kOjwaKZfPZ3tR7azJ3cPfdv1ZWjCUHrF97Jr+41pQiwpNCHe9fn6mFJWXsYPZ/2QXUd3seS2JQzvMjzAERpjAs2SQhOijyml5aV8uuNTZmXMYs7mORw4foBXxr1iCcEYA1hSaBIqni879YupzNgwg33H9tGiWQvGdh/LxD4TuabHNYEO0RjTQFhSaAKyHsji4ukXs+PoDsZ2H8tt/W5jbPexRIRFBDo0Y0wDY0mhkcs+ns3Iv48kKz+L//7ovwzrPCzQIRljGrC6XWRugsr+Y/u58u9Xsv3Idj784YeWEIwxNbIzhUZqw4ENjHtnHIcKDjFnwhwuTbw00CEZY4KAnSk0MvKEsHj7Yoa/OZzS8lKW3b6MUeeNCnRYxpggYWcKjcymezYx5PUh5Bfnk3FPht15bIypEztTaERyi3K5ZuY1NA9tzq6f7bKEYIypMztTaCTKtZyb/3Uz245sY/Gti+nSyp5lbYypOztTaATkCeGFVS/w0ZaPmPa9aVzc9eJAh2SMCVJ2ptAIbLlvC8kvJ3NV96v4yYU/CXQ4xpggZmcKQa5cy5k0ZxLNQpvxyrhXanzIjTHGVMfOFILci6tfZNnuZbx19Vt0atkp0OEYY4KcnSkEsV1Hd/Hw4ocZ020Mt/a7NdDhGGMaAUsKQUpVuXfBvQC8fNXLVm1kjKkXVn0UpEKedPL51FFT6RrbNcDRGGMaC0sKQSi3KJcOLTrQvkV7fjrkp4EOxxjTiFhSCEKvrn2Vfcf2MXvCbMJCbBUaY+qPtSkEmXIt55W1rzCiywgGdxoc6HCMMY2MHWYGmdAnQwH43WW/C3AkxpjGyK9nCiIyWkQ2i8hWEXmoijI/EJEMEdkoIv/0ZzyNwdUXXE18VDzX9bwu0KEYYxohv50piEgo8BJwJZAJrBGRuaqa4VWmO/AwMFxVj4hIW3/F0xhk5mUy79t5/GrYr2ge1jzQ4RhjGiF/nikMBraq6nZVLQZmAldXKnMn8JKqHgFQ1YN+jCfovbb2NVSVHw/6caBDMcY0Uv5MCp2APV7Dme44b+cD54vI5yKyUkRG+5qQiNwlImkikpadne2ncBu2krISXlv3GqO7jSapdVKgwzHGNFL+TAq+brHVSsNhQHfgUmAi8LqIxJ7yJdVXVTVFVVPi4+PrPdBgMGfzHPYd22e9oBpj/MqfSSET6Ow1nABk+SgzR1VLVHUHsBknSZhK/rrmr3Rt1ZUx3cYEOhRjTCPmz6SwBuguIkki0gyYAMytVGY2cBmAiMThVCdt92NMQSkjO4MlO5cwJWUKoSGhgQ7HGNOI+S0pqGopcC/wMbAJeE9VN4rIkyIy3i32MZAjIhnAEuBXqprjr5iCVe+/9qZZaDMmDZgU6FCMMY2cX29eU9X5wPxK4x71eq/AA+7L+HCs+BgxzWK4psc1xEc3zfYUY8zZY91cNHD/+Oof5BfnMyVlSqBDMcY0AZYUGjBV5a9r/sqA9gO4KOGiQIdjjGkCLCk0YJ/v+ZwNBzdwz4X32EN0jDFnhSWFBuylNS/RqnkrJvadGOhQjDFNhCWFBmr/sf3MypjF7f1vJyo8KtDhGGOaCEsKDdTr616npLyEKRdaA7Mx5uyxpNAAlZaX8sraV7jy3Cs5v835gQ7HGNOEWFJogOZvmU9mXiZ3p9wd6FCMMU2MJYUG6JW1r9ChRQe+f/73Ax2KMaaJsaTQwOw6uosFWxZwx4A7aPb7ZoEOxxjTxFhSaGBeX/c6AJMHTkYfq9zTuDHG+JclhQakpKyEN758gzHdx9A1tmugwzHGNEGWFBqQed/OY9+xffa4TWNMwFhSaEDe2/ge7aLbMbb72ECHYoxpoiwpNBCl5aV8vO1jxnYfS1iIX3s0N8aYKllSaCBWZq7kaNFRpq+fHuhQjDFNmCWFBmLBlgWESihHHzwa6FCMMU2YJYUGYv7W+aR2SaVVRKtAh2KMacIsKTQAe/P2sn7/emtgNsYEnCWFBmDh1oUAlhSMMQFnSaEBmL91Pp1bdqZ3fO9Ah2KMaeIsKQRYcVkxn2z7hDHdxtgjN40xAWdJIcC+2PMF+cX5vLru1UCHYowxlhQCbcGWBYSHhJP3UF6gQzHGGEsKgbZw20JSu6QS0zwm0KEYY4wlhUDKys/iqwNfMbrb6ECHYowxgCWFgKq4FNWSgjGmofBrUhCR0SKyWUS2ishDPj7/kYhki8h69zXZn/E0NAu3LqRjTEf6tu0b6FCMMQYAv3XHKSKhwEvAlUAmsEZE5qpqRqWi76rqvf6Ko6EqLS/l/Yz3mdR/kl2KaoxpMPx5pjAY2Kqq21W1GJgJXO3H+QWVVZmrAKs6MsY0LP5MCp2APV7Dme64yq4Xka9E5AMR6exrQiJyl4ikiUhadna2P2I96xZuXUiohHLleVcGOhRjjPHwZ1LwVSdS+Un084BEVU0GFgF/8zUhVX1VVVNUNSU+Pr6ewwyMJTuXkNIxhdiI2ECHYowxHv5MCpmA95F/ApDlXUBVc1T1hDv4GjDIj/E0GEWlRazJWsOILiMCHYoxxpzEn0lhDdBdRJJEpBkwAZjrXUBEOngNjgc2+TGeBiMtK43ismJGdLWkYIxpWPx29ZGqlorIvcDHQCjwpqpuFJEngTRVnQv8VETGA6XAYeBH/oqnIVm+ezkAwzoPC3AkxhhzMr8+IV5V5wPzK4171Ov9w8DD/oyhIVq2exk943oSFxUX6FCMMeYkdkfzWVau5Xy++3NSu6QGOhRjjDmFJYWzbOPBjeSeyLWkYIxpkCwpnGUV7QmWFIwxDZElhbNs+Z7ldGjRgaTYpECHYowxp6hVUhCR80Skufv+UhH5qYjYXVenYfnu5aR2SbX+jowxDVJtzxRmAWUi0g14A0gC/um3qBqp3bm72Z2726qOjDENVm2TQrmqlgLXAtNU9edAhxq+Yyr5ZNsnANy/8P4AR2KMMb7VNimUiMhE4DbgQ3dcuH9CarwWbF1Ap5hOlD9aHuhQjDHGp9omhduBocAfVHWHiCQB//BfWI1PSVkJn2z/hDHdxlh7gjGmwarVHc3ug3F+CiAirYEYVX3Kn4E1NisyV5B3Io8x3ccEOhRjjKlSba8+WioiLUXkHCAdmC4iz/g3tMZlwZYFhIWEMfLckYEOxRhjqlTb6qNWqpoHXAdMV9VBgO3d6mD+1vkM7zycls1bBjoUY4ypUm2TQpjbzfUP+K6h2dTS3ry9fHXgK8Z0s6ojY0zDVtuk8CROF9jbVHWNiJwLbPFfWI3Lwq0LAaw9wRjT4NW2ofl94H2v4e3A9f4KqrFZuG0hnWI60bdt30CHYowx1aptQ3OCiPxbRA6KyAERmSUiCf4OrjFQVZbuXMrIc0fapajGmAavttVH03EepdkR6ATMc8eZGmzO2cyhgkP2PGZjTFCobVKIV9Xpqlrqvt4C4v0YV6NhXWUbY4JJbZPCIRG5RURC3dctQI4/A2sslu9eTnxUPOe3OT/QoRhjTI1qmxQm4VyOuh/YB9yA0/WFqYF1lW2MCSa1SgqqultVx6tqvKq2VdVrcG5kM9WQJ4RtR7ZZ1ZExJmicyZPXHqi3KBqp9254D8AamY0xQeNMkoLVh9Rg+e7lRIVH0b99/0CHYowxtXImSUHrLYpGatnuZVyUcBHhofboCWNMcKg2KYhIvojk+Xjl49yzYKqQdyKP9APpVnVkjAkq1XZzoaoxZyuQxmZl5krKtdwamY0xQeVMqo9qJCKjRWSziGwVkYeqKXeDiKiIpPgznrNp3b51AKR0bDSLZIxpAvyWFEQkFHgJGAP0AiaKSC8f5WJwnuq2yl+xBEL6gXQSYxOJjYgNdCjGGFNr/jxTGAxsVdXtqloMzASu9lHud8CfgSI/xnLWpe9Pp1+7foEOwxhj6sSfSaETsMdrONMd5yEiA4DOqlrtg3tE5C4RSRORtOzs7PqPtJ4VlhSyOWezJQVjTNDxZ1LwdR+D5zJWEQkBngV+UdOEVPVVVU1R1ZT4+IbfD9/XB7+mXMvt/gRjTNDxZ1LIBDp7DScAWV7DMUAfYKmI7AQuAuY2hsbm9fvXA9CvvZ0pGGOCiz+Twhqgu4gkiUgzYALOMxkAUNXgAVWdAAAYcElEQVRcVY1T1URVTQRWAuNVNc2PMZ0Vd314FzHNYkiMTQx0KMYYUye1ehzn6VDVUhG5F+fZzqHAm6q6UUSeBNJUdW71UwheFfcmhIhfr/g1xph657ekAKCq84H5lcY9WkXZS/0Zy9lSruWk70/n1n63BjoUY4ypMzuUrWc7j+4kvzjfrjwyxgQlSwr1LH1/OoBdeWSMCUqWFOrZ+v3rCZEQ+rTtE+hQjDGmziwp1LP0A+mc3+Z8IsMjAx2KMcbUmSWFeqSqrN231toTjDFBy5JCPdp2ZBuZeZlc0vWSQIdijDGnxZJCPVq0fREAI88dGeBIjDHm9FhSqCfyhLBo+yK6tOpCt3O6BTocY4w5LZYU6knp/5Ty6Y5PGZk0EhFffQEaY0zDZ0mhnny5/0uOFB2xqiNjTFCzpFBPKtoTLk+6PMCRGGPM6bOkUE8WbV9Ecrtk2rVoF+hQjDHmtFlSqAeFJYUs372ckUlWdWSMCW6WFOrB8t3LOVF2wtoTjDFBz5JCPViycwlhIWFc3PXiQIdijDFnxJJCPVi2exmDOgwiull0oEMxxpgzYknhDBWVFrF672pGdBkR6FCMMeaMWVI4Q6v3rqa4rJgRXS0pGGOCnyWFM3TJW07nd8M7Dw9wJMYYc+b8+ozmpuB7532PzLxM2kS1CXQoxhhzxuxM4QyUlZfxxZ4v7KojY0yjYUnhDKQfSCe/ON8amY0xjYYlhTPw2a7PAKyR2RjTaFhSOAPLdi8jMTaRhJYJgQ7FGGPqhSWF06SqLN+93KqOjDGNiiWF07TtyDYOHj9IapfUQIdijDH1xpLCaZAnhBV7VgAwNGFogKMxxpj649ekICKjRWSziGwVkYd8fH63iGwQkfUislxEevkznvqijykrMlcQ0yyGXvFBEbIxxtSK35KCiIQCLwFjgF7ARB87/X+qal9V7Q/8GXjGX/HUty/2fMGQhCGEhoQGOhRjjKk3/jxTGAxsVdXtqloMzASu9i6gqnleg9GA+jGeepN/Ip8NBzdY1ZExptHxZzcXnYA9XsOZwJDKhUTkHuABoBng8wHHInIXcBdAly5d6j3QulqTtYZyLbekYIxpdPx5piA+xp1yJqCqL6nqecCDwCO+JqSqr6pqiqqmxMfH13OYdVfRyHxRwkUBjsQYY+qXP5NCJtDZazgByKqm/EzgGj/GU29WZK6gR1wPWke2DnQoxhhTr/yZFNYA3UUkSUSaAROAud4FRKS71+BVwBY/xlMvVJWVmSsZljAs0KEYY0y981ubgqqWisi9wMdAKPCmqm4UkSeBNFWdC9wrIiOBEuAIcJu/4qkvWw5vIacwh6GdrT3BGNP4+PV5Cqo6H5hfadyjXu/v9+f8/cFuWjPGNGZ2R3MdyBPCkp1LiI2IpWd8z0CHY4wx9c6evFYHxY8U0+4v7Rh/wXhCxPKpMabxsT1bHSzZuYQjRUe4vuf1gQ7FGGP8wpJCHXyQ8QEtmrVg1HmjAh2KMcb4hSWFWiorL2P2N7MZd/44IsIiAh2OMcb4hSWFWlq2exnZBdlWdWSMadQsKdTSrIxZRIZFMqbbmECHYowxfmNJoRbKtZxZm2YxuttooptFBzocY4zxG0sKtbB+/3r2HdvHtT2uDXQoxhjjV5YUaiBPCEt3LgXg8iSfPXsbY0yjYUmhBvqYsnTnUrqf051OLTsFOhxjjPErSwo1KCsv47Ndn3Fp4qWBDsUYY/zOurmowfr968k9kctliZcFOhRjzlhJSQmZmZkUFRUFOhTjJxERESQkJBAeHn5a37ekUIOK9oRLEi8JbCDG1IPMzExiYmJITExExNfDEU0wU1VycnLIzMwkKSnptKZh1UdVkCecf5ilu5Zyfpvz6RjTMcARGXPmioqKaNOmjSWERkpEaNOmzRmdCVpSqII+ppSWl/LZrs+s6sg0KpYQGrczXb+WFKqxfv968k7kWSOzMabJsKRQDU97QldrTzCmPuTk5NC/f3/69+9P+/bt6dSpk2e4uLi4VtO4/fbb2bx5c7VlXnrpJWbMmFEfIde7Rx55hGnTpp0y/rbbbiM+Pp7+/fsHIKrvWENzNeZ9O4+ecT3pENMh0KEY0yi0adOG9evXA/D444/TokULfvnLX55URlVRVUJCfB+zTp8+vcb53HPPPWce7Fk2adIk7rnnHu66666AxmFJoQoZ2Rl8tusz/jTyT4EOxRi/+NnCn7F+//p6nWb/9v2ZNvrUo+CabN26lWuuuYbU1FRWrVrFhx9+yBNPPMG6desoLCzkpptu4tFHnce7p6am8uKLL9KnTx/i4uK4++67WbBgAVFRUcyZM4e2bdvyyCOPEBcXx89+9jNSU1NJTU3l008/JTc3l+nTpzNs2DCOHz/OrbfeytatW+nVqxdbtmzh9ddfP+VI/bHHHmP+/PkUFhaSmprKyy+/jIjw7bffcvfdd5OTk0NoaCj/+te/SExM5I9//CPvvPMOISEhjBs3jj/84Q+1+g0uueQStm7dWuffrr5Z9ZGXiiuOAF5e8zLNQptxe//bAxiRMU1HRkYGd9xxB19++SWdOnXiqaeeIi0tjfT0dD755BMyMjJO+U5ubi6XXHIJ6enpDB06lDfffNPntFWV1atX8/TTT/Pkk08C8MILL9C+fXvS09N56KGH+PLLL31+9/7772fNmjVs2LCB3NxcFi5cCMDEiRP5+c9/Tnp6Ol988QVt27Zl3rx5LFiwgNWrV5Oens4vfvGLevp1zh47U/CijykAx4qP8fZXb/OD3j8gPjo+wFEZ4x+nc0TvT+eddx4XXnihZ/idd97hjTfeoLS0lKysLDIyMujVq9dJ34mMjGTMGKc7+0GDBrFs2TKf077uuus8ZXbu3AnA8uXLefDBBwHo168fvXv39vndxYsX8/TTT1NUVMShQ4cYNGgQF110EYcOHeL73/8+4NwwBrBo0SImTZpEZGQkAOecc87p/BQBZUnBh39u+Cd5J/KYkjIl0KEY02RER3/XLf2WLVt47rnnWL16NbGxsdxyyy0+r71v1qyZ531oaCilpaU+p928efNTyqhqjTEVFBRw7733sm7dOjp16sQjjzziicPXpZ+qGvSX/Fr1USWqystpL5PcLpmhCUMDHY4xTVJeXh4xMTG0bNmSffv28fHHH9f7PFJTU3nvvfcA2LBhg8/qqcLCQkJCQoiLiyM/P59Zs2YB0Lp1a+Li4pg3bx7g3BRYUFDAqFGjeOONNygsLATg8OHD9R63v1lSqOSDjA9Yv389U1KmBH3GNyZYDRw4kF69etGnTx/uvPNOhg8fXu/zuO+++9i7dy/JyclMnTqVPn360KpVq5PKtGnThttuu40+ffpw7bXXMmTIEM9nM2bMYOrUqSQnJ5Oamkp2djbjxo1j9OjRpKSk0L9/f5599lmf83788cdJSEggISGBxMREAG688UZGjBhBRkYGCQkJvPXWW/W+zLUhtTmFakhSUlI0LS2t3qYnT4inLeHLfV+SOj2V5HbJLL1tKc3DmtfbfIxpCDZt2kTPnj0DHUaDUFpaSmlpKREREWzZsoVRo0axZcsWwsKCv1bd13oWkbWqmlLTd/269CIyGngOCAVeV9WnKn3+ADAZKAWygUmqusufMXnm7SaDioSwL38f42eOp01kG2bfNNsSgjGN3LFjx7jiiisoLS1FVXnllVcaRUI4U377BUQkFHgJuBLIBNaIyFxV9a64+xJIUdUCEZkC/Bm4yV8xeatIBgDFZcVc/971HC48zOeTPqddi3ZnIwRjTADFxsaydu3aQIfR4PizTWEwsFVVt6tqMTATuNq7gKouUdUCd3AlkODHeKr0y//8khWZK3jr6rfo3z6wt5gbY0wg+TMpdAL2eA1nuuOqcgewwNcHInKXiKSJSFp2dnY9hggzv57JC6tf4OcX/Zwbe99Yr9M2xphg48+k4OvSHZ+t2iJyC5ACPO3rc1V9VVVTVDUlPr7+bib75tA3TJ47meGdh1t3FsYYg3+TQibQ2Ws4AciqXEhERgK/Bcar6gk/xnNSNxYFJQXc+P6NRIVH8e4N7xIeenqPrjPGmMbEn0lhDdBdRJJEpBkwAZjrXUBEBgCv4CSEg36MBTi5cfm++fex8eBG/nHdP+jUsrpaLWNMfbn00ktPuRFt2rRp/OQnP6n2ey1atAAgKyuLG264ocpp13S5+rRp0ygoKPAMjx07lqNHj9Ym9LNq6dKljBs37pTxL774It26dUNEOHTokF/m7bekoKqlwL3Ax8Am4D1V3SgiT4rIeLfY00AL4H0RWS8ic6uYXL16O/1t3lz/Jr8d8VtGnTfqbMzSGIPTidzMmTNPGjdz5kwmTpxYq+937NiRDz744LTnXzkpzJ8/n9jY2NOe3tk2fPhwFi1aRNeuXf02D7/e0ayq81X1fFU9T1X/4I57VFXnuu9Hqmo7Ve3vvsZXP8Uzl1uUy08X/JSLu17MY5c+5u/ZGdMoeFe9nokbbriBDz/8kBMnnJrinTt3kpWVRWpqque+gYEDB9K3b1/mzJlzyvd37txJnz59AKcLigkTJpCcnMxNN93k6VoCYMqUKaSkpNC7d28ee8z5P3/++efJysrisssu47LLnEfsJiYmeo64n3nmGfr06UOfPn08D8HZuXMnPXv25M4776R3796MGjXqpPlUmDdvHkOGDGHAgAGMHDmSAwcOAM69ELfffjt9+/YlOTnZ003GwoULGThwIP369eOKK66o9e83YMAAzx3QflPxQItgeQ0aNEjPxFPLnlIeR9dmrT2j6RgTjDIyMgIdgo4dO1Znz56tqqr/+7//q7/85S9VVbWkpERzc3NVVTU7O1vPO+88LS8vV1XV6OhoVVXdsWOH9u7dW1VVp06dqrfffruqqqanp2toaKiuWbNGVVVzcnJUVbW0tFQvueQSTU9PV1XVrl27anZ2tieWiuG0tDTt06ePHjt2TPPz87VXr166bt063bFjh4aGhuqXX36pqqo33nij/v3vfz9lmQ4fPuyJ9bXXXtMHHnhAVVV//etf6/33339SuYMHD2pCQoJu3779pFi9LVmyRK+66qoqf8PKy1GZr/UMpGkt9rFNqu+jotIinl35LKPOG8XADgMDHY4xTZJ3FZJ31ZGq8pvf/Ibk5GRGjhzJ3r17PUfcvnz22WfccsstACQnJ5OcnOz57L333mPgwIEMGDCAjRs3+uzsztvy5cu59tpriY6OpkWLFlx33XWebriTkpI8D97x7nrbW2ZmJt/73vfo27cvTz/9NBs3bgScrrS9nwLXunVrVq5cycUXX0xSUhLQ8LrXblJJ4W/r/8aB4wd4aPhDgQ7FmCbrmmuuYfHixZ6nqg0c6BygzZgxg+zsbNauXcv69etp166dz+6yvfnqtHLHjh385S9/YfHixXz11VdcddVVNU5Hq+kDrqLbbai6e+777ruPe++9lw0bNvDKK6945qc+utL2Na4haTJJobS8lD9/8WcGdxrMpYmXBjocY5qsFi1acOmllzJp0qSTGphzc3Np27Yt4eHhLFmyhF27qu8G7eKLL2bGjBkAfP3113z11VeA0+12dHQ0rVq14sCBAyxY8N09sTExMeTn5/uc1uzZsykoKOD48eP8+9//ZsSIEbVeptzcXDp1cq5i/Nvf/uYZP2rUKF588UXP8JEjRxg6dCj//e9/2bFjB9DwutduMklhVsYsth/ZzsOpDzfoLG1MUzBx4kTS09OZMGGCZ9zNN99MWloaKSkpzJgxgx49elQ7jSlTpnDs2DGSk5P585//zODBgwHnKWoDBgygd+/eTJo06aRut++66y7GjBnjaWiuMHDgQH70ox8xePBghgwZwuTJkxkwYECtl+fxxx/3dH0dFxfnGf/II49w5MgR+vTpQ79+/ViyZAnx8fG8+uqrXHfddfTr14+bbvLd3dvixYs93WsnJCSwYsUKnn/+eRISEsjMzCQ5OZnJkyfXOsbaajJdZ3/07Ue8/uXrzPrBLEKkyeRCY05iXWc3DQ226+yG5Krzr+Kq868KdBjGGNOg2SGzMcYYD0sKxjQxwVZlbOrmTNevJQVjmpCIiAhycnIsMTRSqkpOTg4RERGnPY0m06ZgjMFz5Up9P5fENBwREREkJJz+88osKRjThISHh3vupDXGF6s+MsYY42FJwRhjjIclBWOMMR5Bd0eziGQD1XeKcqo4wD+PKTr7bFkaJluWhqsxLc+ZLEtXVa3xIfdBlxROh4ik1eb27mBgy9Iw2bI0XI1pec7Gslj1kTHGGA9LCsYYYzyaSlJ4NdAB1CNblobJlqXhakzL4/dlaRJtCsYYY2qnqZwpGGOMqQVLCsYYYzwadVIQkdEisllEtorIQ4GOpy5EpLOILBGRTSKyUUTud8efIyKfiMgW92/rQMdaWyISKiJfisiH7nCSiKxyl+VdEWkW6BhrS0RiReQDEfnGXUdDg3XdiMjP3W3saxF5R0QigmXdiMibInJQRL72GudzPYjjeXd/8JWIDAxc5KeqYlmedrexr0Tk3yIS6/XZw+6ybBaR79VXHI02KYhIKPASMAboBUwUkV6BjapOSoFfqGpP4CLgHjf+h4DFqtodWOwOB4v7gU1ew38CnnWX5QhwR0CiOj3PAQtVtQfQD2e5gm7diEgn4KdAiqr2AUKBCQTPunkLGF1pXFXrYQzQ3X3dBbx8lmKsrbc4dVk+AfqoajLwLfAwgLsvmAD0dr/zV3efd8YabVIABgNbVXW7qhYDM4GrAxxTranqPlVd577Px9npdMJZhr+5xf4GXBOYCOtGRBKAq4DX3WEBLgc+cIsE07K0BC4G3gBQ1WJVPUqQrhuc3pIjRSQMiAL2ESTrRlU/Aw5XGl3VergaeFsdK4FYEelwdiKtma9lUdX/qGqpO7gSqOgT+2pgpqqeUNUdwFacfd4Za8xJoROwx2s40x0XdEQkERgArALaqeo+cBIH0DZwkdXJNODXQLk73AY46rXBB9P6ORfIBqa71WGvi0g0QbhuVHUv8BdgN04yyAXWErzrBqpeD8G+T5gELHDf+21ZGnNSEB/jgu76WxFpAcwCfqaqeYGO53SIyDjgoKqu9R7to2iwrJ8wYCDwsqoOAI4TBFVFvrj17VcDSUBHIBqnmqWyYFk31QnabU5EfotTpTyjYpSPYvWyLI05KWQCnb2GE4CsAMVyWkQkHCchzFDVf7mjD1Sc8rp/DwYqvjoYDowXkZ041XiX45w5xLpVFhBc6ycTyFTVVe7wBzhJIhjXzUhgh6pmq2oJ8C9gGMG7bqDq9RCU+wQRuQ0YB9ys391Y5rdlacxJYQ3Q3b2KohlOo8zcAMdUa26d+xvAJlV9xuujucBt7vvbgDlnO7a6UtWHVTVBVRNx1sOnqnozsAS4wS0WFMsCoKr7gT0icoE76goggyBcNzjVRheJSJS7zVUsS1CuG1dV62EucKt7FdJFQG5FNVNDJSKjgQeB8apa4PXRXGCCiDQXkSScxvPV9TJTVW20L2AsTov9NuC3gY6njrGn4pwOfgWsd19jceriFwNb3L/nBDrWOi7XpcCH7vtz3Q15K/A+0DzQ8dVhOfoDae76mQ20DtZ1AzwBfAN8DfwdaB4s6wZ4B6ctpATn6PmOqtYDTpXLS+7+YAPOFVcBX4YalmUrTttBxT7g/7zK/9Zdls3AmPqKw7q5MMYY49GYq4+MMcbUkSUFY4wxHpYUjDHGeFhSMMYY42FJwRhjjIclBWNcIlImIuu9XvV2l7KIJHr3fmlMQxVWcxFjmoxCVe0f6CCMCSQ7UzCmBiKyU0T+JCKr3Vc3d3xXEVns9nW/WES6uOPbuX3fp7uvYe6kQkXkNffZBf8RkUi3/E9FJMOdzswALaYxgCUFY7xFVqo+usnrszxVHQy8iNNvE+77t9Xp634G8Lw7/nngv6raD6dPpI3u+O7AS6raGzgKXO+OfwgY4E7nbn8tnDG1YXc0G+MSkWOq2sLH+J3A5aq63e2kcL+qthGRQ0AHVS1xx+9T1TgRyQYSVPWE1zQSgU/UefALIvIgEK6qvxeRhcAxnO4yZqvqMT8vqjFVsjMFY2pHq3hfVRlfTni9L+O7Nr2rcPrkGQSs9eqd1JizzpKCMbVzk9ffFe77L3B6fQW4GVjuvl8MTAHPc6lbVjVREQkBOqvqEpyHEMUCp5ytGHO22BGJMd+JFJH1XsMLVbXistTmIrIK50Bqojvup8CbIvIrnCex3e6Ovx94VUTuwDkjmILT+6UvocA/RKQVTi+ez6rzaE9jAsLaFIypgdumkKKqhwIdizH+ZtVHxhhjPOxMwRhjjIedKRhjjPGwpGCMMcbDkoIxxhgPSwrGGGM8LCkYY4zx+P9445lktTmD8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step\n",
      "1500/1500 [==============================] - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3096214653015137, 0.7242666666984559]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.30416401831309, 0.7180000001589457]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0630 01:13:19.462395 140611292423936 deprecation.py:506] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.9646 - acc: 0.1519 - val_loss: 1.9365 - val_acc: 0.1720\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9458 - acc: 0.1641 - val_loss: 1.9262 - val_acc: 0.1990\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9406 - acc: 0.1679 - val_loss: 1.9167 - val_acc: 0.2060\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.9296 - acc: 0.1809 - val_loss: 1.9077 - val_acc: 0.2330\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.9207 - acc: 0.1895 - val_loss: 1.8986 - val_acc: 0.2350\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.9101 - acc: 0.2044 - val_loss: 1.8879 - val_acc: 0.2500\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.9073 - acc: 0.1951 - val_loss: 1.8771 - val_acc: 0.2520\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8964 - acc: 0.2099 - val_loss: 1.8651 - val_acc: 0.2510\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8763 - acc: 0.2312 - val_loss: 1.8508 - val_acc: 0.2640\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8694 - acc: 0.2305 - val_loss: 1.8357 - val_acc: 0.2740\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8551 - acc: 0.2355 - val_loss: 1.8185 - val_acc: 0.2880\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.8526 - acc: 0.2409 - val_loss: 1.8021 - val_acc: 0.3060\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8321 - acc: 0.2665 - val_loss: 1.7813 - val_acc: 0.3280\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8201 - acc: 0.2625 - val_loss: 1.7607 - val_acc: 0.3550\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8022 - acc: 0.2733 - val_loss: 1.7390 - val_acc: 0.3700\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7875 - acc: 0.2851 - val_loss: 1.7158 - val_acc: 0.3930\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7690 - acc: 0.2891 - val_loss: 1.6950 - val_acc: 0.4060\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7537 - acc: 0.3015 - val_loss: 1.6729 - val_acc: 0.4340\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.7398 - acc: 0.2980 - val_loss: 1.6484 - val_acc: 0.4420\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7184 - acc: 0.3197 - val_loss: 1.6252 - val_acc: 0.4640\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.7048 - acc: 0.3256 - val_loss: 1.6015 - val_acc: 0.4680\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6864 - acc: 0.3385 - val_loss: 1.5773 - val_acc: 0.4820\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6654 - acc: 0.3445 - val_loss: 1.5533 - val_acc: 0.5100\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6454 - acc: 0.3477 - val_loss: 1.5297 - val_acc: 0.5250\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6317 - acc: 0.3539 - val_loss: 1.5070 - val_acc: 0.5270\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6119 - acc: 0.3731 - val_loss: 1.4848 - val_acc: 0.5410\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6051 - acc: 0.3727 - val_loss: 1.4662 - val_acc: 0.5400\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5815 - acc: 0.3867 - val_loss: 1.4459 - val_acc: 0.5540\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5565 - acc: 0.4057 - val_loss: 1.4222 - val_acc: 0.5560\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5442 - acc: 0.3883 - val_loss: 1.4007 - val_acc: 0.5590\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5329 - acc: 0.4096 - val_loss: 1.3797 - val_acc: 0.5610\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5149 - acc: 0.4092 - val_loss: 1.3609 - val_acc: 0.5750\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4993 - acc: 0.4285 - val_loss: 1.3410 - val_acc: 0.5850\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4808 - acc: 0.4281 - val_loss: 1.3232 - val_acc: 0.5870\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4657 - acc: 0.4369 - val_loss: 1.3049 - val_acc: 0.6050\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4552 - acc: 0.4412 - val_loss: 1.2866 - val_acc: 0.6020\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4475 - acc: 0.4492 - val_loss: 1.2699 - val_acc: 0.6090\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4313 - acc: 0.4532 - val_loss: 1.2537 - val_acc: 0.6150\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4157 - acc: 0.4571 - val_loss: 1.2356 - val_acc: 0.6200\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3921 - acc: 0.4723 - val_loss: 1.2207 - val_acc: 0.6300\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3923 - acc: 0.4637 - val_loss: 1.2052 - val_acc: 0.6360\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3715 - acc: 0.4759 - val_loss: 1.1907 - val_acc: 0.6380\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3639 - acc: 0.4820 - val_loss: 1.1765 - val_acc: 0.6510\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3516 - acc: 0.4827 - val_loss: 1.1637 - val_acc: 0.6510\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3476 - acc: 0.4825 - val_loss: 1.1528 - val_acc: 0.6580\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3344 - acc: 0.4936 - val_loss: 1.1366 - val_acc: 0.6570\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3133 - acc: 0.5000 - val_loss: 1.1224 - val_acc: 0.6620\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3103 - acc: 0.5000 - val_loss: 1.1098 - val_acc: 0.6600\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2902 - acc: 0.5099 - val_loss: 1.0959 - val_acc: 0.6780\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2819 - acc: 0.5149 - val_loss: 1.0839 - val_acc: 0.6760\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2793 - acc: 0.5147 - val_loss: 1.0736 - val_acc: 0.6790\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2654 - acc: 0.5199 - val_loss: 1.0633 - val_acc: 0.6830\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2499 - acc: 0.5264 - val_loss: 1.0519 - val_acc: 0.6820\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2487 - acc: 0.5208 - val_loss: 1.0425 - val_acc: 0.6960\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2397 - acc: 0.5351 - val_loss: 1.0327 - val_acc: 0.6980\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2399 - acc: 0.5329 - val_loss: 1.0216 - val_acc: 0.7030\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2126 - acc: 0.5500 - val_loss: 1.0078 - val_acc: 0.7010\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2115 - acc: 0.5441 - val_loss: 1.0013 - val_acc: 0.7040\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2083 - acc: 0.5425 - val_loss: 0.9910 - val_acc: 0.7060\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1997 - acc: 0.5467 - val_loss: 0.9837 - val_acc: 0.7060\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1969 - acc: 0.5585 - val_loss: 0.9768 - val_acc: 0.7100\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1778 - acc: 0.5596 - val_loss: 0.9660 - val_acc: 0.7100\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1697 - acc: 0.5613 - val_loss: 0.9591 - val_acc: 0.7050\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1545 - acc: 0.5713 - val_loss: 0.9509 - val_acc: 0.7140\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1696 - acc: 0.5675 - val_loss: 0.9433 - val_acc: 0.7150\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1429 - acc: 0.5733 - val_loss: 0.9325 - val_acc: 0.7110\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1265 - acc: 0.5776 - val_loss: 0.9246 - val_acc: 0.7120\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1290 - acc: 0.5757 - val_loss: 0.9156 - val_acc: 0.7220\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1182 - acc: 0.5792 - val_loss: 0.9054 - val_acc: 0.7200\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1198 - acc: 0.5788 - val_loss: 0.9009 - val_acc: 0.7240\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1017 - acc: 0.5885 - val_loss: 0.8913 - val_acc: 0.7220\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1021 - acc: 0.5935 - val_loss: 0.8874 - val_acc: 0.7200\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1006 - acc: 0.5927 - val_loss: 0.8857 - val_acc: 0.7220\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0871 - acc: 0.5920 - val_loss: 0.8774 - val_acc: 0.7210\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0847 - acc: 0.5973 - val_loss: 0.8715 - val_acc: 0.7200\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0824 - acc: 0.5945 - val_loss: 0.8638 - val_acc: 0.7210\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0757 - acc: 0.5957 - val_loss: 0.8610 - val_acc: 0.7270\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0725 - acc: 0.6051 - val_loss: 0.8567 - val_acc: 0.7270\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0628 - acc: 0.6044 - val_loss: 0.8478 - val_acc: 0.7230\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0663 - acc: 0.6013 - val_loss: 0.8426 - val_acc: 0.7260\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0486 - acc: 0.6129 - val_loss: 0.8345 - val_acc: 0.7320\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0580 - acc: 0.6104 - val_loss: 0.8331 - val_acc: 0.7310\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0354 - acc: 0.6085 - val_loss: 0.8242 - val_acc: 0.7320\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0337 - acc: 0.6128 - val_loss: 0.8200 - val_acc: 0.7330\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0327 - acc: 0.6148 - val_loss: 0.8175 - val_acc: 0.7280\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0288 - acc: 0.6161 - val_loss: 0.8127 - val_acc: 0.7290\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0122 - acc: 0.6271 - val_loss: 0.8068 - val_acc: 0.7320\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0119 - acc: 0.6305 - val_loss: 0.8030 - val_acc: 0.7330\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0162 - acc: 0.6244 - val_loss: 0.8001 - val_acc: 0.7370\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0066 - acc: 0.6249 - val_loss: 0.7947 - val_acc: 0.7340\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0119 - acc: 0.6239 - val_loss: 0.7908 - val_acc: 0.7430\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9916 - acc: 0.6312 - val_loss: 0.7845 - val_acc: 0.7420\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9912 - acc: 0.6367 - val_loss: 0.7805 - val_acc: 0.7430\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9914 - acc: 0.6264 - val_loss: 0.7780 - val_acc: 0.7410\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9696 - acc: 0.6397 - val_loss: 0.7713 - val_acc: 0.7430\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9752 - acc: 0.6313 - val_loss: 0.7674 - val_acc: 0.7390\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9870 - acc: 0.6320 - val_loss: 0.7655 - val_acc: 0.7420\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9581 - acc: 0.6452 - val_loss: 0.7619 - val_acc: 0.7380\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9748 - acc: 0.6371 - val_loss: 0.7623 - val_acc: 0.7440\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9685 - acc: 0.6421 - val_loss: 0.7596 - val_acc: 0.7410\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9495 - acc: 0.6529 - val_loss: 0.7521 - val_acc: 0.7420\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9430 - acc: 0.6507 - val_loss: 0.7500 - val_acc: 0.7420\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9408 - acc: 0.6453 - val_loss: 0.7469 - val_acc: 0.7440\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9516 - acc: 0.6527 - val_loss: 0.7452 - val_acc: 0.7410\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9467 - acc: 0.6523 - val_loss: 0.7416 - val_acc: 0.7460\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9256 - acc: 0.6545 - val_loss: 0.7385 - val_acc: 0.7440\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9408 - acc: 0.6509 - val_loss: 0.7350 - val_acc: 0.7440\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9189 - acc: 0.6615 - val_loss: 0.7307 - val_acc: 0.7460\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9253 - acc: 0.6572 - val_loss: 0.7276 - val_acc: 0.7450\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9186 - acc: 0.6621 - val_loss: 0.7234 - val_acc: 0.7510\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9318 - acc: 0.6488 - val_loss: 0.7233 - val_acc: 0.7500\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9094 - acc: 0.6559 - val_loss: 0.7220 - val_acc: 0.7460\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9184 - acc: 0.6640 - val_loss: 0.7187 - val_acc: 0.7470\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9102 - acc: 0.6613 - val_loss: 0.7162 - val_acc: 0.7500\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9020 - acc: 0.6701 - val_loss: 0.7128 - val_acc: 0.7490\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9049 - acc: 0.6593 - val_loss: 0.7113 - val_acc: 0.7500\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9032 - acc: 0.6629 - val_loss: 0.7068 - val_acc: 0.7470\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8968 - acc: 0.6675 - val_loss: 0.7053 - val_acc: 0.7480\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9070 - acc: 0.6616 - val_loss: 0.7051 - val_acc: 0.7490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8960 - acc: 0.6669 - val_loss: 0.7035 - val_acc: 0.7470\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8997 - acc: 0.6703 - val_loss: 0.7008 - val_acc: 0.7510\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8761 - acc: 0.6733 - val_loss: 0.6980 - val_acc: 0.7480\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8899 - acc: 0.6631 - val_loss: 0.6955 - val_acc: 0.7500\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8902 - acc: 0.6664 - val_loss: 0.6941 - val_acc: 0.7490\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8737 - acc: 0.6805 - val_loss: 0.6907 - val_acc: 0.7450\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8801 - acc: 0.6747 - val_loss: 0.6921 - val_acc: 0.7500\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8704 - acc: 0.6879 - val_loss: 0.6871 - val_acc: 0.7510\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8790 - acc: 0.6744 - val_loss: 0.6866 - val_acc: 0.7500\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8704 - acc: 0.6799 - val_loss: 0.6831 - val_acc: 0.7510\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8650 - acc: 0.6787 - val_loss: 0.6821 - val_acc: 0.7490\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8577 - acc: 0.6845 - val_loss: 0.6799 - val_acc: 0.7490\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8548 - acc: 0.6913 - val_loss: 0.6787 - val_acc: 0.7490\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8594 - acc: 0.6856 - val_loss: 0.6782 - val_acc: 0.7480\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8681 - acc: 0.6783 - val_loss: 0.6778 - val_acc: 0.7490\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8423 - acc: 0.6933 - val_loss: 0.6732 - val_acc: 0.7490\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8480 - acc: 0.6869 - val_loss: 0.6698 - val_acc: 0.7500\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8368 - acc: 0.6907 - val_loss: 0.6678 - val_acc: 0.7500\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8588 - acc: 0.6837 - val_loss: 0.6691 - val_acc: 0.7520\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8491 - acc: 0.6855 - val_loss: 0.6673 - val_acc: 0.7540\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8380 - acc: 0.6872 - val_loss: 0.6678 - val_acc: 0.7520\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8341 - acc: 0.6968 - val_loss: 0.6655 - val_acc: 0.7520\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8517 - acc: 0.6909 - val_loss: 0.6640 - val_acc: 0.7580\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8494 - acc: 0.6868 - val_loss: 0.6628 - val_acc: 0.7540\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8385 - acc: 0.6936 - val_loss: 0.6613 - val_acc: 0.7520\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8260 - acc: 0.6939 - val_loss: 0.6585 - val_acc: 0.7540\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8346 - acc: 0.6927 - val_loss: 0.6576 - val_acc: 0.7580\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8312 - acc: 0.6983 - val_loss: 0.6570 - val_acc: 0.7490\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8244 - acc: 0.6916 - val_loss: 0.6542 - val_acc: 0.7480\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8239 - acc: 0.6981 - val_loss: 0.6532 - val_acc: 0.7580\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8251 - acc: 0.6903 - val_loss: 0.6519 - val_acc: 0.7480\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8311 - acc: 0.6975 - val_loss: 0.6517 - val_acc: 0.7450\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8216 - acc: 0.6969 - val_loss: 0.6508 - val_acc: 0.7530\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8194 - acc: 0.6943 - val_loss: 0.6492 - val_acc: 0.7580\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8108 - acc: 0.7001 - val_loss: 0.6488 - val_acc: 0.7600\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8116 - acc: 0.7044 - val_loss: 0.6466 - val_acc: 0.7540\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8068 - acc: 0.7067 - val_loss: 0.6464 - val_acc: 0.7510\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7995 - acc: 0.7044 - val_loss: 0.6435 - val_acc: 0.7560\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7992 - acc: 0.7073 - val_loss: 0.6422 - val_acc: 0.7550\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7981 - acc: 0.7049 - val_loss: 0.6407 - val_acc: 0.7520\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8065 - acc: 0.6995 - val_loss: 0.6390 - val_acc: 0.7560\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7871 - acc: 0.7100 - val_loss: 0.6382 - val_acc: 0.7560\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7940 - acc: 0.7112 - val_loss: 0.6384 - val_acc: 0.7550\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8011 - acc: 0.7033 - val_loss: 0.6390 - val_acc: 0.7520\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7899 - acc: 0.7044 - val_loss: 0.6356 - val_acc: 0.7560\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7924 - acc: 0.7104 - val_loss: 0.6342 - val_acc: 0.7520\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7886 - acc: 0.7080 - val_loss: 0.6326 - val_acc: 0.7570\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.7911 - acc: 0.7045 - val_loss: 0.6315 - val_acc: 0.7590\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7831 - acc: 0.7076 - val_loss: 0.6312 - val_acc: 0.7580\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7897 - acc: 0.7103 - val_loss: 0.6307 - val_acc: 0.7540\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7745 - acc: 0.7096 - val_loss: 0.6307 - val_acc: 0.7570\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.7803 - acc: 0.7091 - val_loss: 0.6293 - val_acc: 0.7520\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7836 - acc: 0.7077 - val_loss: 0.6273 - val_acc: 0.7590\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7816 - acc: 0.7112 - val_loss: 0.6260 - val_acc: 0.7580\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7675 - acc: 0.7179 - val_loss: 0.6248 - val_acc: 0.7610\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7677 - acc: 0.7099 - val_loss: 0.6256 - val_acc: 0.7570\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7827 - acc: 0.7053 - val_loss: 0.6242 - val_acc: 0.7540\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7662 - acc: 0.7160 - val_loss: 0.6232 - val_acc: 0.7580\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7698 - acc: 0.7179 - val_loss: 0.6207 - val_acc: 0.7570\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7715 - acc: 0.7165 - val_loss: 0.6217 - val_acc: 0.7560\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7607 - acc: 0.7187 - val_loss: 0.6196 - val_acc: 0.7580\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7604 - acc: 0.7188 - val_loss: 0.6205 - val_acc: 0.7600\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7462 - acc: 0.7245 - val_loss: 0.6167 - val_acc: 0.7610\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7581 - acc: 0.7231 - val_loss: 0.6149 - val_acc: 0.7650\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7647 - acc: 0.7232 - val_loss: 0.6144 - val_acc: 0.7640\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7640 - acc: 0.7167 - val_loss: 0.6156 - val_acc: 0.7590\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7672 - acc: 0.7136 - val_loss: 0.6150 - val_acc: 0.7570\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7448 - acc: 0.7208 - val_loss: 0.6119 - val_acc: 0.7610\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7565 - acc: 0.7143 - val_loss: 0.6133 - val_acc: 0.7620\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7604 - acc: 0.7252 - val_loss: 0.6136 - val_acc: 0.7530\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7616 - acc: 0.7201 - val_loss: 0.6111 - val_acc: 0.7620\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7414 - acc: 0.7260 - val_loss: 0.6113 - val_acc: 0.7570\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7427 - acc: 0.7219 - val_loss: 0.6094 - val_acc: 0.7620\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7411 - acc: 0.7276 - val_loss: 0.6097 - val_acc: 0.7540\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7445 - acc: 0.7207 - val_loss: 0.6089 - val_acc: 0.7590\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7410 - acc: 0.7204 - val_loss: 0.6068 - val_acc: 0.7620\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7557 - acc: 0.7193 - val_loss: 0.6075 - val_acc: 0.7640\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7393 - acc: 0.7232 - val_loss: 0.6068 - val_acc: 0.7650\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7279 - acc: 0.7307 - val_loss: 0.6043 - val_acc: 0.7620\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7444 - acc: 0.7300 - val_loss: 0.6054 - val_acc: 0.7620\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7410 - acc: 0.7204 - val_loss: 0.6041 - val_acc: 0.7620\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n",
      "1500/1500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44953240927060445, 0.8355999999682109]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6567809325853984, 0.745333333492279]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5750258494615554, 0.805]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
